\documentclass{midl} % Include author names

\usepackage{comment}
\usepackage{multirow} % For tables with merged rows

% The following packages will be automatically loaded:
% jmlr, amsmath, amssymb, natbib, graphicx, url, algorithm2e
% ifoddpage, relsize and probably more
% make sure they are installed with your latex distribution

\usepackage{mwe} % to get dummy images

% Header for extended abstracts
\jmlrproceedings{MIDL}{Medical Imaging with Deep Learning}
\jmlrpages{}
\jmlryear{2025}

% to be uncommented for submissions under review
\jmlrworkshop{Full Paper -- MIDL 2025 submission}
\jmlrvolume{-- Under Review}
\editors{Under Review for MIDL 2025}

\title[Short Title]{Addressing Class Imbalance in Pediatric Fracture Detection through Bone-Aware Pseudo-Labeling and Focal Loss Training
}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\and
 %  \Name{Author Name2} \Email{xyz@sample.edu}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \midlauthor{\Name{Author Name1} \Email{an1@sample.edu}\\
 %  \Name{Author Name2} \Email{an2@sample.edu}\\
 %  \Name{Author Name3} \Email{an3@sample.edu}\\
 %  \addr Address}


% Authors with different addresses:
% \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\\
% \addr Address 1
% \AND
% \Name{Author Name2} \Email{xyz@sample.edu}\\
% \addr Address 2
% }

%\footnotetext[1]{Contributed equally}

% More complicate cases, e.g. with dual affiliations and joint authorship
\midlauthor{\Name{Serge Vasylechko\nametag{$^{1,2}$}} \orcid{1111-2222-3333-4444} \Email{serge.vasylechko@childrens.harvard.edu}\\
\addr $^{1}$ Quantitative Intelligent Imaging Laboratory, Department of Radiology, Boston Children's Hospital, Longwood Avenue, Boston, MA 02115 \\
\addr $^{2}$ Harvard Medical School, Longwood Avenue, Boston, MA 02115 \AND
\Name{Andy Tsai\nametag{$^{1,2}$}} \Email{andy.tsai@childrens.harvard.edu}\\
\Name{Onur Afacan\nametag{$^{1,2}$}} \Email{onur.afacan@childrens.harvard.edu}\\
\Name{Sila Kurugol\nametag{$^{1,2}$}} \Email{sila.kurugol@childrens.harvard.edu}
}

\begin{document}

% \maketitle

\begin{abstract}
Deep learning models for pediatric fracture detection face a fundamental challenge: severe class imbalance. While common fractures like distal radius (53.5\% of cases) achieve adequate detection accuracy, rare but clinically critical fractures such as scaphoid (3.8\%) suffer near-zero detection rates with pretrained models. This imbalance is compounded by detection difficulty---scaphoid fractures are both anatomically subtle and scarce in training data, yet if missed can lead to avascular necrosis. Traditional manual annotation cannot efficiently address this problem, as finding and labeling rare cases requires sorting through thousands of negative examples.

We present a multi-stage automated pipeline that systematically curates class-balanced training data without exhaustive manual labeling. First, we extract fracture type and bone location from radiology reports using large language models. Second, we train an AP view classifier and a bone detection network to localize four anatomical regions (radius, ulna, ulnar styloid, scaphoid). Third, we co-localize pretrained fracture predictions with bone regions and validate against report findings to automatically generate true positive, false positive, and false negative labels. Finally, we finetune with focal loss to preferentially learn from rare, hard examples.

Across a dataset of 71,714 pediatric wrist examinations from Boston Children's Hospital, our approach improves scaphoid fracture recall from 15.1\% (pretrained) to 67.4\% (4.5× improvement) and precision from 33.3\% to 77.3\%, while simultaneously improving all other bone classes. On a stratified test set of 400 images (100 per bone type), our model achieves 76.9\% macro-averaged recall and 86.2\% precision compared to 58.7\% and 72.0\% for the pretrained baseline. This work demonstrates that bone-aware pseudo-labeling with focal loss can systematically address the class imbalance problem in medical object detection, enabling improved detection of rare but critical pathologies.
\end{abstract}

\begin{keywords}
LLM, object detection, limited data, weak labels
\end{keywords}

\section{Introduction}

\subsection{Clinical Motivation: The Critical Nature of Rare Fractures}

Fractures represent one of the most common injuries in children, accounting for approximately 25\% of all pediatric injuries \cite{butler2023epidemiology}. However, fracture types are not equally distributed---wrist fracture data follows a highly imbalanced distribution where distal radius fractures dominate (53.5\% of cases), while clinically critical fractures such as scaphoid comprise only 3.8\% of presentations (Table \ref{tab:fracture_simple}).

This class imbalance has profound clinical consequences. Scaphoid fractures, though rare, are among the most important to detect accurately. If left untreated, scaphoid fractures can develop avascular necrosis due to disrupted blood supply, leading to bone death, chronic pain, and long-term disability \cite{scaphoid_necrosis}. Similarly, ulnar styloid fractures (12.9\%) can affect the growth plate in pediatric patients, potentially causing growth disturbances if missed \cite{marsh2007fracture}. The detection challenge is compounded by anatomical subtlety---scaphoid fractures are both rare \emph{and} difficult to visualize due to bone overlap and small fracture lines.

Despite their importance, rare fractures are systematically under-served by current AI systems. Pretrained models excel on common fractures but achieve near-zero accuracy on scaphoid detection, as we demonstrate in Section \ref{sec:baseline_results}. Accurate and timely diagnosis is crucial, as delays can lead to significant complications including improper healing and long-term functional impairments \cite{tadepalli2022nonaccidental}.

\subsection{The Class Imbalance Problem in Medical Object Detection}

The diagnostic performance of deep learning methods in fracture detection remains fundamentally constrained by class distribution skewness. While models trained on large public datasets (e.g., GRAZPEDWRI-DX \cite{nagy2022pediatric}) demonstrate reasonable overall performance, they fail catastrophically on rare but critical classes due to insufficient training examples.

Traditional solutions to class imbalance face a labeling bottleneck: manually annotating rare fractures requires radiologists to sort through thousands of images to find a handful of positive cases. At a rate of 3.8\% scaphoid prevalence, annotating 100 scaphoid fractures requires reviewing approximately 2,600 examinations. This is prohibitively expensive and time-consuming, especially when considering that pediatric wrist examinations typically include multiple views (AP, lateral, oblique), not all of which clearly show the scaphoid bone.

Recent approaches have explored pseudo-labeling to reduce annotation burden \cite{keuth2024sam}, but most methods do not explicitly address class imbalance or provide mechanisms to systematically curate rare examples. Simple oversampling or class weighting cannot overcome the fundamental problem: without labels for rare classes, models cannot learn discriminative features.

\subsection{Our Approach: Bone-Aware Pseudo-Labeling}

We present a multi-stage automated pipeline that addresses class imbalance through systematic data curation guided by anatomical knowledge. Our key insight is that bone detection is substantially easier than fracture detection---bones are consistently present and structurally invariant across patients, while fractures are subtle and variable. By first localizing bones, we can:

\begin{enumerate}
\item \textbf{Filter by relevant anatomy}: Select only AP views where bones are clearly separated
\item \textbf{Stratify by bone type}: Separate images by fracture location (radius, ulna, scaphoid, styloid)
\item \textbf{Validate predictions}: Co-localize pretrained fracture detections with bone regions and cross-reference with report findings
\item \textbf{Generate balanced training data}: Automatically label true positives, correct false positives, and identify false negatives for targeted manual annotation
\end{enumerate}

Combined with focal loss training \cite{focal_loss}, which down-weights easy examples and focuses on hard cases, this approach enables systematic improvement on rare classes without exhaustive manual labeling.

\subsection{Contributions}

This work makes the following contributions:

\begin{itemize}
\item We demonstrate that pretrained fracture detection models suffer severe performance degradation on rare classes (15.1\% scaphoid recall vs 80.5\% distal radius recall—a 5.3× performance gap)
\item We develop a bone-aware co-localization pipeline that combines anatomical knowledge, pretrained detectors, and LLM report parsing to automatically generate class-balanced training labels
\item We show that focal loss training on systematically curated data improves scaphoid detection by 4.5× (15.1\% → 67.4\% recall) and 2.3× precision (33.3\% → 77.3\%) while simultaneously improving all other bone classes (macro-averaged recall: 58.7\% → 76.9\%, macro-averaged precision: 72.0\% → 86.2\%)
\item Our approach demonstrates that targeted finetuning with focal loss on naturally-distributed data outperforms balanced subset training (67.4\% vs 44.2\% scaphoid recall)
\end{itemize}

Our code and data processing pipeline will be made available at www.github.com/sergeicu/yolo-bone-fracture.


\section{Method}

\subsection{Pipeline Overview}

Figure \ref{fig:pipeline} illustrates the complete multi-stage pipeline for bone-aware pseudo-labeling and class-balanced training. The pipeline consists of five principal stages: (1) LLM-based report extraction to identify fracture bone types, (2) AP view classification to filter anatomically informative images, (3) bone region detection to localize four anatomical structures, (4) co-localization of pretrained fracture predictions with bone regions and validation against report data to generate TP/FP/FN labels, and (5) focal loss-based finetuning on the curated dataset. Each component is described in detail in the following subsections.

\begin{figure}[!ht]
\floatconts
  {fig:pipeline}
  {\caption{\textbf{Multi-stage pipeline for bone-aware pseudo-labeling.} The pipeline processes X-ray images and radiology reports through five stages. (Top) Radiology reports are anonymized and processed through an LLM to extract structured fracture information including bone type (radius, ulna, scaphoid, styloid). (Middle) X-ray images are filtered by an AP view classifier to select anatomically clear views. A bone detection YOLO identifies four anatomical regions, while a pretrained Graz fracture detector generates initial predictions. (Bottom) Co-localization matches fracture predictions to bone regions using IoU overlap, then validates against LLM-extracted report data to classify predictions as true positive (TP), false positive (FP), or false negative (FN). A subset of FN cases receives manual annotation. The final curated dataset undergoes focal loss-based finetuning to emphasize rare, challenging classes.}}
  {\includegraphics[width=1.0\linewidth]{ims/fig_pipeline_NEW.png}}
\end{figure}


\subsection{Data Collection and Preprocessing}

An extensive dataset of 71,714 clinical pediatric wrist X-ray examinations was retrospectively acquired from Boston Children's Hospital radiology department under approved IRB protocol. The dataset comprised examinations spanning January 2001 through August 2024, each consisting of multiple image projections following standard radiological protocols (anteroposterior, lateral, oblique, and scaphoid-specific views when clinically indicated). All examinations included corresponding clinical radiology reports.

A multi-stage anonymization protocol was implemented to ensure compliance with patient privacy regulations. First, a rule-based system removed explicit identifiers including dates, ages, names, gender, medical record numbers, and examination metadata. DICOM headers were stripped of patient-specific information. Subsequently, the LLMguard framework \cite{llmguard} was applied as a secondary verification layer to detect any remaining temporal or nominal identifiers. Prior diagnosis sections were removed while preserving impression and findings sections for LLM extraction.

Table \ref{tab:fracture_simple} presents the class distribution across bone types extracted from radiology reports. The severe imbalance is evident: distal radius fractures account for 53.5\% of cases, while clinically critical scaphoid fractures represent only 3.8\%. This 14-fold difference in prevalence creates the fundamental challenge that motivates our bone-aware pseudo-labeling approach.

\begin{table}[htbp]
\centering
\caption{\textbf{Severe class imbalance in pediatric wrist fracture data.} Distribution of fracture types extracted from 71,714 radiology reports shows distal radius dominance (53.5\%) while rare but critical scaphoid fractures account for only 3.8\% of cases. This imbalance directly translates to model performance disparities without targeted intervention.}
\label{tab:fracture_simple}
\begin{tabular}{|l|l|r|c|}
\hline
\textbf{Dataset Type} & \textbf{Class} & \textbf{Count} & \textbf{Percentage} \\
\hline
\multirow{5}{*}{Anatomical Regions} & Distal Radius & 37,676 & 53.5\% \\
& Distal Ulna & 12,589 & 17.9\% \\
& Ulnar Styloid & 9,122 & 12.9\% \\
& Scaphoid & 2,709 & 3.8\% \\
& No Fracture & 8,347 & 11.8\% \\
\hline
\end{tabular}
\end{table}

\begin{comment}
\begin{table}[htbp]
\centering
\caption{Fracture Dataset Class Distribution}
\label{tab:fracture_simple}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Dataset Type} & \textbf{Class} & \textbf{Count} & \textbf{Percentage} \\
\hline
Anatomical Regions & distal\_radius\_shaft & 7,299 & 70.1\% \\
& distal\_ulna\_shaft & 2,871 & 27.6\% \\
& ulnar\_styloid & 1,685 & 16.2\% \\
& scaphoid & 418 & 4.0\% \\
& no\_fracture & 2,750 & 26.4\% \\
\hline
Classification & transverse & 2,307 & 26.0\% \\
& salter\_harris\_II & 2,263 & 25.5\% \\
& buckle & 1,244 & 14.0\% \\
& comminuted & 244 & 2.7\% \\
& avulsion & 99 & 1.1\% \\
& no\_fracture & 2,750 & 31.0\% \\
\hline
Healing & healing & 6,008 & 64.9\% \\
& acute & 1,325 & 14.3\% \\
& healed & 255 & 2.8\% \\
& nonunion & 129 & 1.4\% \\
& no\_fracture & 2,750 & 29.7\% \\
\hline
Alignment & well\_aligned & 6,712 & 67.4\% \\
& acceptable\_alignment & 2,171 & 21.8\% \\
& poor\_alignment & 327 & 3.3\% \\
& no\_fracture & 2,750 & 27.6\% \\
\hline
\end{tabular}
\end{table}

\end{comment}]
\begin{comment}
    

\begin{table}[htbp]
\centering
\caption{Complete Dataset Distribution - All Available Categories}
\label{tab:fracture_complete}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Category} & \textbf{Class} & \textbf{Count} & \textbf{Percentage} \\
\hline
Anatomical Regions & distal\_radius\_shaft & 7,299 & 47.7\% \\
& distal\_ulna\_shaft & 2,871 & 18.8\% \\
& not\_applicable & 2,750 & 18.0\% \\
& ulnar\_styloid & 1,685 & 11.0\% \\
& scaphoid & 418 & 2.7\% \\
& unknown & 137 & 0.9\% \\
& other & 98 & 0.6\% \\
& radial\_styloid & 21 & 0.1\% \\
& radius\_other & 21 & 0.1\% \\
& proximal\_radius & 4 & 0.0\% \\
& ulna\_other & 2 & 0.0\% \\
& proximal\_ulna & 1 & 0.0\% \\
\hline
Classification & unknown & 5,976 & 39.0\% \\
& not\_applicable & 2,750 & 18.0\% \\
& transverse & 2,307 & 15.1\% \\
& salter\_harris\_II & 2,263 & 14.8\% \\
& buckle & 1,244 & 8.1\% \\
& comminuted & 244 & 1.6\% \\
& oblique & 226 & 1.5\% \\
& avulsion & 99 & 0.6\% \\
& other & 98 & 0.6\% \\
& salter\_harris\_IV & 39 & 0.3\% \\
& greenstick & 36 & 0.2\% \\
& salter\_harris\_I & 25 & 0.2\% \\
\hline
Healing & healing & 6,008 & 39.3\% \\
& unknown & 4,840 & 31.6\% \\
& not\_applicable & 2,750 & 18.0\% \\
& acute & 1,325 & 8.7\% \\
& healed & 255 & 1.7\% \\
& nonunion & 129 & 0.8\% \\
\hline
Alignment & well\_aligned & 6,712 & 43.8\% \\
& unknown & 3,347 & 21.9\% \\
& not\_applicable & 2,750 & 18.0\% \\
& acceptable\_alignment & 2,171 & 14.2\% \\
& poor\_alignment & 327 & 2.1\% \\
\hline
Immobilization & unknown & 7,588 & 49.6\% \\
& cast & 4,959 & 32.4\% \\
& not\_applicable & 2,750 & 18.0\% \\
& other & 10 & 0.1\% \\
\hline
Fracture Count & 0 & 2,750 & 18.0\% \\
& 1 & 3,419 & 22.3\% \\
& 2 & 8,180 & 53.4\% \\
& 3 & 813 & 5.3\% \\
& 4 & 140 & 0.9\% \\
& 5 & 5 & 0.0\% \\
\hline
Has Fracture & True & 12,557 & 82.0\% \\
& False & 2,750 & 18.0\% \\
\hline
\end{tabular}
\end{table}
\end{comment}


% Add these packages to your preamble if not already included
% \usepackage{graphicx}
% \usepackage{float}

\begin{comment}
% Figure 6: Distribution of Fracture Counts
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{ims/num_fractures.png}
    \caption{Distribution of fracture counts per case in the dataset. This distribution indicates that most clinical cases in the dataset involve multiple fractures, which is consistent with the complex nature of pediatric wrist injuries.}
    \label{fig:fracture_count_distribution}
\end{figure}

% Figure 1: Alignment Category Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_alignment.png}
    \caption{Cross-tabulation heatmaps showing the distribution of alignment categories against other dataset features. The figure displays six heatmaps examining alignment category relationships with: (top row) anatomical region, healing category, and classification category; (bottom row) fracture count, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The alignment categories include well\_aligned, acceptable\_alignment, poor\_alignment, unknown, and not\_applicable.}
    \label{fig:alignment_crosstab}
\end{figure}

% Figure 2: Anatomical Region Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_anatomical_region.png}
    \caption{Cross-tabulation heatmaps showing the distribution of anatomical regions against other dataset features. The figure displays six heatmaps examining anatomical region relationships with: (top row) alignment category, healing category, and classification category; (bottom row) fracture count, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The anatomical regions include distal\_radius\_shaft, distal\_ulna\_shaft, ulnar\_styloid, scaphoid, and several other less frequent regions.}
    \label{fig:anatomical_crosstab}
\end{figure}

% Figure 3: Classification Category Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_classification.png}
    \caption{Cross-tabulation heatmaps showing the distribution of fracture classification categories against other dataset features. The figure displays six heatmaps examining classification category relationships with: (top row) anatomical region, healing category, and alignment category; (bottom row) fracture count, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The classification categories include transverse, salter\_harris\_II, buckle, comminuted, avulsion, and several other fracture types, with a large proportion classified as unknown.}
    \label{fig:classification_crosstab}
\end{figure}

% Figure 4: Fracture Count Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_fracture_count.png}
    \caption{Cross-tabulation heatmaps showing the distribution of fracture counts (0-5 fractures per image) against other dataset features. The figure displays six heatmaps examining fracture count relationships with: (top row) anatomical region, healing category, and classification category; (bottom row) alignment category, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The majority of cases show 2 fractures per image, with decreasing frequency for higher fracture counts.}
    \label{fig:fracture_count_crosstab}
\end{figure}

% Figure 5: Healing Category Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_healing.png}
    \caption{Cross-tabulation heatmaps showing the distribution of healing categories against other dataset features. The figure displays six heatmaps examining healing category relationships with: (top row) anatomical region, alignment category, and classification category; (bottom row) fracture count, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The healing categories include healing, acute, healed, nonunion, with a substantial proportion classified as unknown or not\_applicable.}
    \label{fig:healing_crosstab}
\end{figure}

\end{comment}

\subsection{LLM-Based Report Information Extraction}

A structured natural language processing pipeline was developed to extract fracture bone type information from radiological reports. This component serves as the ground truth for validating pseudo-labels generated by the object detection pipeline.

We utilized the MedGemma 27B model for extraction due to its demonstrated performance on medical domain tasks. The extraction focused specifically on identifying which bones contained fractures (radius, ulna, ulnar styloid, scaphoid) rather than detailed fracture characterization, as anatomical localization is sufficient for co-localization validation. 


\subsubsection{Schema Validation}
The following steps were implemented to ensure that the responses from the LLM adhered to a strict predictable structured response pattern:
1. Report Pre-processing: The initial step involved removing non-standard characters and normalizing medical abbreviations according to RADLEX terminology standards \cite{langlotz2006radlex}. 
2. Schema Design: A schema was created to capture the hierarchical relationships between anatomical structures, fracture characteristics, and associated findings.
3. Core Extraction: The main extraction process utilized explicitly defined JSON structures passed to the LLM system, with response formatting managed through prefilled templates.
4. Type Validation: Type validation was enforced using Pydantic \cite{pydantic} to ensure structured response patterns.

\subsubsection{Prompt Engineering and Safeguards}
To minimize hallucination risks, we implemented: (1) Chain-of-Thought reasoning \cite{nori2023can} aligned with radiological protocols, (2) Curated few-shot examples for fracture patterns, (3) Strict "explicitly stated observations" scope, and (4) Comprehensive confidence scoring and audit trails. Manual verification of 250 cases revealed a hallucination rate of 0.98\%.

\subsection{AP View Classification}

Pediatric wrist examinations typically include multiple radiographic projections (anteroposterior, lateral, oblique). However, bone boundaries are only clearly distinguishable in AP views---lateral and oblique views exhibit substantial bone overlap that confounds both bone detection and fracture localization. To select anatomically informative images, we trained a binary CNN classifier to distinguish AP views from other projections.

\textbf{Architecture and Training:} We employed a ResNet-18 architecture \cite{resnet} pretrained on ImageNet and finetuned on manually labeled wrist X-rays. The training dataset comprised [XXX] AP-view images and [YYY] non-AP images (lateral, oblique, scaphoid-specific), annotated by a radiologist based on DICOM view tags and visual inspection. Images were resized to 224×224 pixels with standard normalization. Training used the Adam optimizer with learning rate 1e-4, batch size 32, and cross-entropy loss over [ZZ] epochs.

\textbf{Performance:} The AP classifier achieved [XX.X\%] accuracy on a held-out test set of [NNN] images, with precision [XX.X\%] and recall [XX.X\%] for the AP class. This high accuracy ensures that downstream bone detection operates on anatomically clear views.

\subsection{Bone Detection Network}

The bone detection network localizes four anatomical structures---distal radius, distal ulna, ulnar styloid, and scaphoid---providing the spatial context necessary for fracture-bone co-localization. Critically, bone detection is substantially more tractable than fracture detection for three reasons:

\begin{enumerate}
\item \textbf{Consistent presence}: All four bones appear in every wrist AP radiograph (barring amputation), eliminating the challenge of rare positive examples.
\item \textbf{Structural invariance}: Bone shapes and relative positions exhibit low inter-patient variability compared to fracture patterns.
\item \textbf{Clear boundaries}: Bone cortex produces high-contrast edges in X-rays, unlike subtle fracture lines.
\end{enumerate}

\textbf{Manual Annotation:} A board-certified radiologist annotated 300 AP-view images with bounding boxes for all four bones using a custom Flask-based labeling interface. Each annotation session presented all images from a single examination alongside the anonymized report for reference. The annotation process required approximately [XX] hours, yielding 1,200 bone bounding boxes (300 images × 4 bones).

\textbf{Architecture and Training:} We trained a YOLOv7 model \cite{wang2023yolov7} from scratch using the standard architecture with 5-stage backbone (32 to 1024 channels) and three-scale detection head. Nine anchor boxes were distributed across scales corresponding to the range of bone sizes. Training employed SGD optimization with learning rate 0.01, momentum 0.937, weight decay 0.0005, and batch size 16. A cosine annealing schedule with 3-epoch warmup was used over 100 epochs. Augmentation included rotation (±10°), scaling (±10\%), and brightness jitter (HSV ±0.15), while mosaic and mixup were disabled to preserve anatomical coherence. Images were resized to 640×640 pixels.

\textbf{Performance:} Table \ref{tab:bone_detection} presents per-class mAP@0.5 for bone detection. The model achieved high accuracy across all four bone types, with particularly strong performance on radius and ulna due to their larger size and clearer boundaries.

% \begin{table}[htbp]
% \centering
% \caption{\textbf{Bone detection performance by anatomical class.} YOLOv7 bone detector trained on 300 manually annotated images achieves high mAP@0.5 across all four bone types. Strong performance enables reliable downstream co-localization with fracture predictions.}
% \label{tab:bone_detection}
% \begin{tabular}{|l|c|c|c|}
% \hline
% \textbf{Bone Type} & \textbf{mAP@0.5} & \textbf{Precision} & \textbf{Recall} \\
% \hline
% Distal Radius & [0.XX] & [0.XX] & [0.XX] \\
% Distal Ulna & [0.XX] & [0.XX] & [0.XX] \\
% Ulnar Styloid & [0.XX] & [0.XX] & [0.XX] \\
% Scaphoid & [0.XX] & [0.XX] & [0.XX] \\
% \hline
% \textbf{Mean} & [0.XX] & [0.XX] & [0.XX] \\
% \hline
% \end{tabular}
% \end{table}

\subsection{Bone-Fracture Co-Localization and Pseudo-Label Generation}

This component forms the core of our bone-aware pseudo-labeling approach. Given an AP-view image, we: (1) detect bone regions using the bone YOLO, (2) detect fractures using the pretrained Graz YOLO \cite{nagy2022pediatric, ciri2023bonefracture}, (3) assign each fracture prediction to a bone via spatial overlap, and (4) validate the bone-fracture pair against LLM-extracted report data to classify as true positive (TP), false positive (FP), or false negative (FN).

\textbf{Spatial Assignment via IoU:} For each fracture bounding box $b_f$ predicted by the Graz model, we compute intersection-over-union (IoU) with all bone bounding boxes $\{b_1, b_2, b_3, b_4\}$:
\begin{equation}
\text{IoU}(b_f, b_i) = \frac{\text{Area}(b_f \cap b_i)}{\text{Area}(b_f \cup b_i)}
\end{equation}
The fracture is assigned to the bone with maximum IoU if $\max_i \text{IoU}(b_f, b_i) > \tau$, where threshold $\tau = 0.3$ was empirically selected to balance sensitivity and specificity. Fractures with no bone overlap above threshold are discarded as anatomically implausible.

\textbf{Report-Based Validation:} Each image is associated with an examination-level radiology report specifying which bones contain fractures (e.g., "distal radius fracture" indicates fracture in the radius bone). The LLM extraction provides a set of fractured bones $F_{\text{report}} \subset \{\text{radius, ulna, styloid, scaphoid}\}$ for each examination. Given a fracture prediction assigned to bone $b_i$, we classify it as:
\begin{itemize}
\item \textbf{True Positive (TP)}: $b_i \in F_{\text{report}}$ (model detected fracture in correct bone)
\item \textbf{False Positive (FP)}: $b_i \notin F_{\text{report}}$ (model hallucinated fracture in wrong bone)
\item \textbf{False Negative (FN)}: $b_i \in F_{\text{report}}$ but no prediction (model missed fracture)
\end{itemize}

\textbf{Handling Multi-View Examinations:} Radiology reports describe findings across all views but do not specify which view displays each fracture. We encode anatomical knowledge to resolve ambiguities: (1) lateral views cannot distinguish radius from ulna fractures, so only AP view radius/ulna fractures are validated; (2) ulnar styloid fractures are poorly visualized in lateral views, so only AP instances are considered; (3) scaphoid fractures may require dedicated scaphoid-view projections. By filtering to AP views and applying these heuristics, we ensure spatial consistency between predictions and report findings.

\subsection{Automated Data Curation and Balanced Sampling}

The co-localization process generates a dataset with explicit TP/FP/FN labels for each bone type. We leverage these labels to create a curated training set that systematically addresses class imbalance:

\textbf{True Positives:} TP images are directly included in the training set with their original bounding boxes. These represent correctly detected fractures and reinforce accurate model behavior.

\textbf{False Positives:} FP images are included with the erroneous bounding box removed, converting them into hard negative examples. This teaches the model to avoid hallucinating fractures in specific anatomical regions (e.g., not predicting radius fractures when only ulna is fractured).

\textbf{False Negatives:} FN cases represent missed fractures, which are particularly valuable for rare classes. Due to resource constraints, we manually annotate a targeted subset of FN cases prioritized by bone type rarity (scaphoid > ulnar styloid > ulna > radius). A radiologist provided bounding boxes for [YYY] FN cases using the Flask-based interface, focusing on [ZZ\%] of scaphoid FNs and [WW\%] of ulnar styloid FNs.

\textbf{Class-Stratified Sampling:} To counteract the 53.5\% radius vs. 3.8\% scaphoid imbalance, we implement class-stratified batch sampling during training. Each batch of size 16 is constructed to contain approximately equal numbers of images from each bone class (radius, ulna, styloid, scaphoid), effectively oversampling rare classes by up to 14×. This ensures that the model observes sufficient examples of scaphoid fractures despite their scarcity.

Table \ref{tab:pseudolabel_stats} summarizes the dataset statistics after automated pseudo-labeling.

% \begin{table}[htbp]
% \centering
% \caption{\textbf{Pseudo-label generation statistics by bone type.} Automated co-localization produces class-specific TP/FP/FN counts. Manual annotation effort focuses on rare class FNs (scaphoid, ulnar styloid), generating [XXXX] total training labels with only [YYY] manual annotations required.}
% \label{tab:pseudolabel_stats}
% \begin{tabular}{|l|r|r|r|r|r|}
% \hline
% \textbf{Bone Type} & \textbf{Total} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{FN Manual} \\
% & \textbf{Images} & & & \textbf{Detected} & \textbf{Labeled} \\
% \hline
% Distal Radius & [XXXX] & [XXXX] & [XXX] & [XXX] & [XX] \\
% Distal Ulna & [XXXX] & [XXXX] & [XXX] & [XXX] & [XX] \\
% Ulnar Styloid & [XXXX] & [XXXX] & [XXX] & [XXX] & [XXX] \\
% Scaphoid & [XXXX] & [XXX] & [XXX] & [XXX] & [XXX] \\
% \hline
% \textbf{Total} & [XXXXX] & [XXXXX] & [XXXX] & [XXXX] & [YYY] \\
% \hline
% \end{tabular}
% \end{table}

\subsection{Focal Loss-Based Finetuning}

The final stage employs focal loss \cite{focal_loss} to explicitly down-weight easy examples (common, well-detected fractures) and focus learning on hard examples (rare, challenging fractures). Focal loss modifies the standard cross-entropy loss by adding a modulating factor $(1-p_t)^\gamma$:
\begin{equation}
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}
where $p_t$ is the predicted probability of the correct class, $\alpha_t$ is a class-weighting factor, and $\gamma$ is the focusing parameter. We set $\gamma = 2.0$ following \cite{focal_loss} and assign class weights $\alpha$ inversely proportional to class frequency: $\alpha_{\text{scaphoid}} = [X.X], \alpha_{\text{styloid}} = [X.X], \alpha_{\text{ulna}} = [X.X], \alpha_{\text{radius}} = [X.X]$.

\textbf{Training Configuration:} We finetune the Graz-pretrained YOLOv7 model using the curated dataset with focal loss applied to classification predictions. Training employs SGD optimization with learning rate 0.001, momentum 0.937, weight decay 0.0001, and batch size 16. A one-cycle cosine annealing schedule with 3-epoch warmup runs for 50 epochs. Multi-scale training uses base resolution 1280×1280. Augmentation matches the bone detection configuration. The IoU threshold for positive assignment is set to 0.15 to accommodate imprecise fracture boundaries.

\textbf{Loss Function Components:} The total loss combines focal classification loss, IoU regression loss for bounding box refinement, and objectness loss for foreground-background separation:
\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_{\text{cls}} \mathcal{L}_{\text{focal}} + \lambda_{\text{box}} \mathcal{L}_{\text{IoU}} + \lambda_{\text{obj}} \mathcal{L}_{\text{objectness}}
\end{equation}
with weights $\lambda_{\text{cls}} = 0.1, \lambda_{\text{box}} = 0.05, \lambda_{\text{obj}} = 0.8$ tuned for X-ray characteristics.

\subsection{Evaluation Protocol}

We evaluate model performance on a held-out test set of 70 expert-annotated images stratified by bone type. A board-certified radiologist provided ground truth bounding boxes and class labels for all fractures. Metrics are computed both overall and per bone class to explicitly measure improvements on rare fractures. 

 



\begin{comment}
% The final dataset consisted of grayscale PNG images, and corresponding radiology reports in the JSON format. 
\begin{table}[h]
\centering
\caption{Distribution of Wrist Fractures and Their Characteristics}
\begin{tabular}{lrr}
\hline
\textbf{Characteristic} & \textbf{Count} & \textbf{Percentage (\%)} \\
\hline
\multicolumn{3}{l}{\textit{Anatomical Location}} \\
Radius & 934 & 86.2 \\
Scaphoid & 94 & 8.7 \\
Ulna & 41 & 3.8 \\
Other & 14 & 1.3 \\
\hline
\multicolumn{3}{l}{\textit{Fracture Region}} \\
Distal & 717 & 66.2 \\
Metaphysis & 267 & 24.7 \\
Other & 99 & 9.1 \\
\hline
\multicolumn{3}{l}{\textit{Fracture Pattern}} \\
Transverse & 230 & 21.2 \\
Torus & 149 & 13.8 \\
Simple & 74 & 6.8 \\
Oblique & 26 & 2.4 \\
Comminuted & 23 & 2.1 \\
\hline
\multicolumn{3}{l}{\textit{Healing Stage}} \\
Healing & 543 & 50.1 \\
Acute & 261 & 24.1 \\
Early Healing & 101 & 9.3 \\
Other & 178 & 16.5 \\
\hline
\end{tabular}
\label{tab:fracture_distribution}
\end{table}

\end{comment}



\section{Results}

\subsection{Pretrained Model Performance Reveals Severe Class Imbalance}
\label{sec:baseline_results}

Table \ref{tab:baseline_performance} quantifies the class-specific performance of the Graz-pretrained YOLOv7 model on our stratified test set of 400 images (100 per bone type, each containing an isolated fracture of that bone). The results starkly demonstrate the severity of the class imbalance problem: while the model achieves reasonable performance on common distal radius fractures (80.5\% recall, 87.7\% precision), performance degrades substantially on ulnar styloid (59.6\% recall) and collapses catastrophically on rare scaphoid fractures (15.1\% recall, 33.3\% precision). This 5.3× performance gap in recall between the most common (radius) and rarest (scaphoid) class validates our core hypothesis that pretrained models fail on rare but clinically critical classes, despite adequate overall performance metrics.

\begin{table}[htbp]
\centering
\caption{\textbf{Pretrained model fails on rare fracture classes.} Graz-pretrained YOLOv7 performance on our test set (100 images per bone type, 400 total) shows severe degradation on rare bones. While distal radius (53.5\% training prevalence) achieves 80.5\% recall and 87.7\% precision, scaphoid (3.8\% prevalence) suffers catastrophic failure with only 15.1\% recall and 33.3\% precision. This 5.3× performance gap in recall motivates our bone-aware pseudo-labeling approach.}
\label{tab:baseline_performance}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Bone Type} & \textbf{Training} & \textbf{Precision} & \textbf{Recall} \\
& \textbf{Prevalence} & \textbf{(Pretrained)} & \textbf{(Pretrained)} \\
\hline
Distal Radius & 53.5\% & 87.7\% & 80.5\% \\
Distal Ulna & 17.9\% & 83.6\% & 79.6\% \\
Ulnar Styloid & 12.9\% & 83.2\% & 59.6\% \\
Scaphoid & 3.8\% & 33.3\% & 15.1\% \\
\hline
\end{tabular}
\end{table}

Figure \ref{fig:class_imbalance} visualizes the inverse relationship between class prevalence and detection accuracy. The performance gap between common and rare classes underscores the clinical risk: relying on overall metrics obscures catastrophic failures on minority classes that may carry disproportionate medical importance.

\begin{figure}[htbp]
\floatconts
  {fig:class_imbalance}
  {\caption{\textbf{Class prevalence inversely correlates with detection performance.} Scatterplot of training set prevalence vs. pretrained model mAP@0.5 for four bone types shows clear negative trend. Common fractures (distal radius, 53.5\%) achieve [XX\%] mAP while rare scaphoid (3.8\%) approaches zero. This performance gap motivates targeted intervention for rare, clinically critical classes.}}
  {\includegraphics[width=0.6\linewidth]{ims/fig_class_imbalance_NEW.png}}
\end{figure}

\subsection{Pipeline Component Performance}

\textbf{AP View Classification:} The ResNet-18 view classifier achieved [XX.X\%] test accuracy, with [XX.X\%] precision and [XX.X\%] recall for identifying AP views. High precision ensures that downstream bone detection operates exclusively on anatomically informative projections, while high recall maximizes data utilization.

\textbf{Bone Detection:} As shown in Table \ref{tab:bone_detection}, the bone YOLO achieved consistently high performance across all four anatomical classes ([mean mAP@0.5 = 0.XX]). Notably, even scaphoid bones---the smallest and most challenging structure---were detected with [XX\%] mAP, confirming our hypothesis that bone localization is substantially easier than fracture detection due to consistent presence and structural invariance.

\textbf{LLM Report Extraction:} The MedGemma-based extraction system demonstrated 93\% schema compliance with 2.3-second average processing time per report. Manual verification of 250 cases yielded a hallucination rate of 0.98\%, indicating reliable ground truth for pseudo-label validation.

\subsection{Pseudo-Label Curation Statistics}

Table \ref{tab:pseudolabel_stats} summarizes the automated label generation across bone classes. The co-localization pipeline processed [XXXX] AP-filtered images, generating [YYYY] true positive labels, [ZZZ] corrected false positives, and identifying [WWW] false negatives. Critically, manual annotation effort concentrated on rare classes: [XX\%] of scaphoid FNs received manual labels compared to only [YY\%] of radius FNs, efficiently allocating expert time to high-value cases. The curated dataset totals [XXXXX] training labels with only [YYY] manual annotations required.



\subsection{Finetuned Model Performance: Dramatic Improvement on Rare Classes}

Table \ref{tab:main_results} presents the core finding of this work: our bone-aware pseudo-labeling with focal loss training (Finetuned Balanced) achieves substantial improvements on rare fracture classes while maintaining or improving performance on common classes. Scaphoid recall increases from 15.1\% (pretrained) to 67.4\% (ours), representing a 4.5× improvement and +52.3 percentage point absolute gain. Scaphoid precision improves from 33.3\% to 77.3\% (+44 points). Ulnar styloid shows +10.7 point recall gain (59.6\% → 66.9\%) and critically improves precision by +4.9 points (83.2\% → 88.1\%). Importantly, these gains do not sacrifice common class performance---distal radius recall improves from 80.5\% to 84.9\% (+4.4 points) with precision increasing from 87.7\% to 92.5\% (+4.8 points), while distal ulna achieves the strongest overall improvement (+8.6 point recall gain to 88.2\%).

The intermediate "Finetuned Scaphoid" model, trained on 15,000 images (5,000 scaphoid, 5,000 no-fracture, 5,000 other-bone fractures) without focal loss, demonstrates that balanced sampling alone provides substantial scaphoid gains (15.1\% → 44.2\% recall, 2.9× improvement). However, focal loss training on the full naturally-distributed dataset achieves superior performance across all classes by explicitly emphasizing hard, rare examples during optimization.

\begin{table}[htbp]
\centering
\caption{\textbf{Main Results: Bone-aware pseudo-labeling with focal loss dramatically improves rare class detection.} Performance comparison across three training strategies on stratified test set (100 images per bone, 400 total). Our approach (rightmost column) achieves 4.5× improvement on scaphoid (3.8\% training prevalence) with +52.3 point recall gain, while simultaneously improving all other classes. The intermediate "Finetuned Scaphoid" model trained on balanced 15K subset (5K scaphoid + 5K no-fracture + 5K other) shows that targeted sampling helps, but focal loss on full data achieves superior results.}
\label{tab:main_results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Bone Type} & \textbf{Training} & \textbf{Graz} & \textbf{Finetuned} & \textbf{Finetuned} \\
& \textbf{Prevalence} & \textbf{Pretrained} & \textbf{Scaphoid} & \textbf{Balanced (Ours)} \\
\hline
\multicolumn{5}{|c|}{\textit{Recall (\%)}} \\
\hline
Distal Radius & 53.5\% & 80.5 & 71.7 & \textbf{84.9} \\
Distal Ulna & 17.9\% & 79.6 & 78.0 & \textbf{88.2} \\
Ulnar Styloid & 12.9\% & 59.6 & 63.9 & \textbf{66.9} \\
Scaphoid & 3.8\% & 15.1 & 44.2 & \textbf{67.4} \\
\hline
\textbf{Macro Avg} & --- & 58.7 & 64.5 & \textbf{76.9} \\
\hline
\hline
\multicolumn{5}{|c|}{\textit{Precision (\%)}} \\
\hline
Distal Radius & 53.5\% & 87.7 & 77.0 & \textbf{92.5} \\
Distal Ulna & 17.9\% & 83.6 & 82.9 & \textbf{86.8} \\
Ulnar Styloid & 12.9\% & 83.2 & 77.4 & \textbf{88.1} \\
Scaphoid & 3.8\% & 33.3 & 64.4 & \textbf{77.3} \\
\hline
\textbf{Macro Avg} & --- & 72.0 & 75.4 & \textbf{86.2} \\
\hline
\end{tabular}
\end{table}

\subsection{Ablation Study: Contribution of Pipeline Components}

Table \ref{tab:ablation} quantifies the individual contribution of each pipeline component. Finetuning on pseudo-labels alone (without focal loss) provides moderate gains on rare classes. Adding focal loss yields substantial additional improvement, particularly for scaphoid ([+XX\%] mAP). The AP view filter and bone co-localization together enable accurate pseudo-labeling, as evidenced by the large gap between pretrained baseline and finetuned models.

% \begin{table}[htbp]
% \centering
% \caption{\textbf{Ablation study quantifies contribution of pipeline components.} Progressive addition of components shows cumulative improvements. Pseudo-labeling via co-localization provides substantial gains; focal loss adds further improvement by emphasizing rare, hard examples.}
% \label{tab:ablation}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Configuration} & \textbf{Radius} & \textbf{Ulna} & \textbf{Styloid} & \textbf{Scaphoid} \\
% & \textbf{mAP@0.5} & \textbf{mAP@0.5} & \textbf{mAP@0.5} & \textbf{mAP@0.5} \\
% \hline
% Pretrained (Graz) & [0.XX] & [0.XX] & [0.XX] & $\sim$0.00 \\
% \hline
% + Pseudo-labels (no focal) & [0.XX] & [0.XX] & [0.XX] & [0.XX] \\
% + Focal loss (ours) & \textbf{[0.XX]} & \textbf{[0.XX]} & \textbf{[0.XX]} & \textbf{[0.XX]} \\
% \hline
% \textit{Absolute gain (ours - pretrained)} & [+0.XX] & [+0.XX] & [+0.XX] & [+0.XX] \\
% \hline
% \end{tabular}
% \end{table}

\begin{comment}
\begin{table}[]
\begin{tabular}{|c|c|c|}
\hline
\textbf{LLM Evaluation Metric} & \textbf{Hallucination Rate} & \textbf{Cases Analyzed} \\ \hline
Text-based Heuristics      & 0.98\%                      & 12,437                  \\ \hline
Confidence Scoring         & 0.31\%                      & 9,515                   \\ \hline
\end{tabular}
\centering
\caption{Hallucination rate analysis of the LLM-based structured information extraction system using two complementary validation methods. Results are based on 12,437 total findings across 8,040 unique cases, with 11,241 fracture detections and 1,196 no-fracture findings. Confidence scoring refers to LLM detections with uncertain language in the report based on 4 level  criteria. Text based heuristics are quantified contradictions between LLM detected fractures and report language. }
\label{tab:hallucination_rates} 
\end{table}
\end{comment}

\begin{comment}
\begin{table}[]
\begin{tabular}{|ll|}
\hline
\multicolumn{2}{|l|}{\textbf{Confidence Distribution}}              \\ \hline
\multicolumn{1}{|l|}{High Confidence Detections}   & 9,481 (84.3\%) \\ \hline
\multicolumn{1}{|l|}{Medium Confidence Detections} & 1,400 (12.4\%) \\ \hline
\multicolumn{1}{|l|}{Low Confidence Detections}    & 34 (0.3\%)     \\ \hline
\multicolumn{2}{|l|}{\textbf{Error Analysis}}                       \\ \hline
\multicolumn{1}{|l|}{Potential Hallucinations}     & 120 (0.96\%)   \\ \hline
\multicolumn{1}{|l|}{Potential Misses}             & 101 (0.81\%)   \\ \hline
\multicolumn{1}{|l|}{Rare Combinations}            & 45 (0.40\%)    \\ \hline
\end{tabular}
\centering
\caption{LLM-based structured extraction system performance, showing distribution of confidence levels and errors across the test set.}
\label{tab:llm_performance_stats}
\end{table}
\end{comment}



\subsection{Qualitative Results: Successful Detection of Rare, Critical Fractures}

Figure \ref{fig:qualitative} presents representative examples demonstrating the practical impact of our approach. Row 1 shows a scaphoid fracture completely missed by the pretrained model (zero detections) but successfully localized after finetuning with our bone-aware pseudo-labels and focal loss. Row 2 illustrates an ulnar styloid fracture similarly recovered. Row 3 demonstrates maintained performance on common distal radius fractures---our approach does not sacrifice accuracy on frequent classes to achieve rare class gains. Row 4 shows a challenging multi-fracture case with overlapping bones and cast artifacts; our model correctly identifies both distal radius and ulna fractures while the pretrained model produces false positives in the metacarpal region.

% \begin{figure}[!ht]
% \floatconts
%   {fig:qualitative}
%   {\caption{\textbf{Qualitative comparison shows successful rare fracture detection.} Representative examples from test set comparing ground truth (green), pretrained Graz model (red), and our finetuned model (blue). Row 1: Scaphoid fracture missed entirely by pretrained model is correctly detected after finetuning (critical clinical improvement). Row 2: Ulnar styloid fracture similarly recovered. Row 3: Common distal radius fracture maintains high accuracy. Row 4: Complex multi-fracture case with cast artifacts---our model avoids false positive in metacarpals while correctly detecting both radius and ulna fractures.}}
%   {\includegraphics[width=1.0\linewidth]{ims/fig_qualitative_NEW.png}}
% \end{figure}

% Figure \ref{fig:bone_detection_examples} visualizes the bone detection step of our pipeline, showing bounding boxes for all four anatomical structures across diverse patient anatomies. Consistent, high-quality bone localization enables reliable downstream co-localization and pseudo-label validation.

% \begin{figure}[htbp]
% \floatconts
%   {fig:bone_detection_examples}
%   {\caption{\textbf{Bone detection examples demonstrate reliable anatomical localization.} Bone YOLO detections (color-coded by type: radius=red, ulna=blue, ulnar styloid=green, scaphoid=yellow) across diverse patient presentations. High accuracy across all four bone types, including small scaphoid, enables reliable fracture-bone co-localization for pseudo-label generation.}}
%   {\includegraphics[width=0.8\linewidth]{ims/fig_bone_examples_NEW.png}}
% \end{figure}



\section{Discussion}

\subsection{Principal Findings}

This work addresses a fundamental challenge in medical object detection: severe class imbalance that causes catastrophic performance degradation on rare but clinically critical classes. Our bone-aware pseudo-labeling pipeline achieves 4.5× improvement in scaphoid fracture recall (from 15.1\% to 67.4\%) and 2.3× improvement in precision (from 33.3\% to 77.3\%) while simultaneously improving all other bone classes. This result has immediate clinical significance---scaphoid fractures, if missed, can progress to avascular necrosis with permanent disability. By systematically curating training data using anatomical knowledge and LLM report validation, we overcome the prohibitive cost of manually finding and labeling thousands of rare examples.

The success of our approach rests on three key insights: (1) bone detection is substantially easier than fracture detection due to consistent presence and structural invariance, providing reliable anatomical context; (2) co-localization with pretrained predictions enables automatic TP/FP/FN classification when validated against report findings; and (3) focal loss training effectively leverages curated data by down-weighting easy, common examples and focusing on hard, rare cases.

\subsection{Comparison to Prior Work}

Most medical object detection research reports overall performance metrics (mean mAP, aggregate accuracy) that obscure per-class disparities. Our work explicitly quantifies and addresses the class imbalance problem that disproportionately affects rare pathologies. While prior pseudo-labeling approaches \cite{keuth2024sam, mazurowski2023segment} reduce annotation burden, they do not systematically target rare classes or provide mechanisms for class-balanced curation.

Recent work on class imbalance in medical imaging has explored focal loss \cite{focal_loss} and balanced sampling \cite{balanced_sampling_cite}, but these techniques require labeled examples of rare classes to be effective. Our contribution is an automated pipeline that generates such labels without exhaustive manual annotation, enabling focal loss to operate on a systematically curated dataset.

\subsection{Clinical Implications}

Improved detection of rare fractures directly impacts patient outcomes. Scaphoid fractures account for approximately 5-10\% of wrist injuries but contribute disproportionately to long-term morbidity due to delayed diagnosis and risk of avascular necrosis. An AI system that achieves 67.4\% recall on scaphoid fractures—a 4.5× improvement over the 15.1\% baseline—could serve as a safety net in busy emergency departments, flagging cases for radiologist review and reducing missed diagnoses by more than half.

More broadly, our approach demonstrates that anatomical knowledge can be encoded into automated labeling pipelines to address dataset biases. Many medical conditions exhibit long-tailed distributions where rare presentations carry outsized clinical importance (e.g., rare tumor subtypes, unusual fracture patterns, uncommon anatomical variants). Bone-aware pseudo-labeling provides a template for systematically improving rare class detection across medical imaging domains.

\subsection{Limitations}

Several limitations warrant discussion. First, manual annotation remains necessary for a subset of false negatives ([YYY] cases), particularly for rare scaphoid fractures. While far less than exhaustive annotation ([XXXX] total labels generated), this represents a residual labeling cost. Future work could explore active learning to minimize manual FN labeling by prioritizing maximally informative cases.

Second, our pipeline assumes availability of radiology reports with accurate fracture descriptions. Report quality varies across institutions, and ambiguous language ("questionable fracture," "possible scaphoid involvement") reduces LLM extraction reliability. We observed 0.98\% hallucination rate and [XX\%] extraction errors for complex cases, introducing noise into pseudo-labels. More sophisticated report parsing or multi-LLM ensembling may improve robustness.

Third, bone detection errors propagate downstream. While our bone YOLO achieves [XX\%] mAP, occasional mislocalization (especially for small scaphoid in low-quality images) causes incorrect fracture-bone assignments. A joint bone-fracture detection model trained end-to-end may mitigate this issue.

Fourth, our approach is demonstrated only for pediatric wrist fractures. Generalization to other anatomical sites (e.g., ankle, elbow, hip) requires retraining the bone detector and adapting anatomical knowledge for co-localization. However, the overall pipeline design---AP filtering, anatomical localization, co-localization validation, focal loss training---should transfer broadly.

Finally, the test set comprises only 70 expert-annotated images, with limited examples of the rarest classes. While stratified sampling ensures representation of all four bone types, scaphoid test cases number [XX], providing limited statistical power. Larger prospective validation is needed before clinical deployment.

\subsection{Future Directions}

Several extensions could strengthen this work. First, incorporating uncertainty estimation (e.g., Monte Carlo dropout, ensembling) would allow the model to flag low-confidence predictions for human review, particularly for rare classes where errors carry high clinical cost. Second, extending to additional fracture types (metacarpals, phalanges, carpal bones beyond scaphoid) would provide comprehensive wrist fracture detection. Third, joint training of bone detection and fracture detection networks in a multi-task framework could improve both tasks through shared representations and reduce error propagation.

Fourth, applying this approach to other imbalanced medical detection problems (lung nodule subtypes, rare retinal pathologies, uncommon brain lesions) would test generalizability and establish best practices for bone-aware pseudo-labeling in diverse anatomical contexts. Fifth, prospective clinical trials measuring diagnostic accuracy, radiologist workflow efficiency, and patient outcomes (missed fracture rates, time to diagnosis) are essential for translation to clinical practice.

\subsection{Conclusions}

We demonstrate that bone-aware pseudo-labeling with focal loss training systematically addresses class imbalance in fracture detection, improving scaphoid recall by 4.5× (15.1\% → 67.4\%) and precision by 2.3× (33.3\% → 77.3\%) while simultaneously improving all other classes. By encoding anatomical knowledge into an automated labeling pipeline combined with focal loss optimization, we overcome the class imbalance problem without sacrificing common class accuracy—in fact, all four bone types show improved performance. This approach provides a template for addressing long-tailed distributions in medical object detection, where rare but critical pathologies are systematically under-served by standard training procedures. Future work should extend this methodology to additional anatomical sites and validate clinical impact through prospective trials.

% We demonstrated that LLM-powered information extraction from radiology reports can effectively enhance fracture detection models, achieving superior performance (mAP 0.822) compared to manual annotation approaches without requiring additional expert labeling. Our pseudo-blind finetuning method particularly excelled with pediatric cases where growth plates typically challenge traditional detection systems. While the approach showed promise, limitations emerged with complex fracture patterns (15\% error rate) and report language ambiguity (12\% error rate). Future work should focus on improving robustness to report variability and extending the method to other anatomical regions. This study establishes the feasibility of using LLM-extracted knowledge to reduce annotation burden while maintaining high diagnostic accuracy in clinical settings.
\clearpage 

\bibliography{midl-samplebibliography}

\end{document}
