\documentclass{midl} % Include author names

\usepackage{comment}
\usepackage{multirow} % For tables with merged rows

% The following packages will be automatically loaded:
% jmlr, amsmath, amssymb, natbib, graphicx, url, algorithm2e
% ifoddpage, relsize and probably more
% make sure they are installed with your latex distribution

\usepackage{mwe} % to get dummy images

% Header for extended abstracts
\jmlrproceedings{MIDL}{Medical Imaging with Deep Learning}
\jmlrpages{}
\jmlryear{2025}

% to be uncommented for submissions under review
\jmlrworkshop{Full Paper -- MIDL 2025 submission}
\jmlrvolume{-- Under Review}
\editors{Under Review for MIDL 2025}

\title[Short Title]{Enhancing Wrist Fracture Detection through LLM-Powered Data Extraction and Multi-Modal Pseudo-Labeling
}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
 % \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\and
 %  \Name{Author Name2} \Email{xyz@sample.edu}\\
 %  \addr Address}

 % Three or more authors with the same address:
 % \midlauthor{\Name{Author Name1} \Email{an1@sample.edu}\\
 %  \Name{Author Name2} \Email{an2@sample.edu}\\
 %  \Name{Author Name3} \Email{an3@sample.edu}\\
 %  \addr Address}


% Authors with different addresses:
% \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\\
% \addr Address 1
% \AND
% \Name{Author Name2} \Email{xyz@sample.edu}\\
% \addr Address 2
% }

%\footnotetext[1]{Contributed equally}

% More complicate cases, e.g. with dual affiliations and joint authorship
\midlauthor{\Name{Serge Vasylechko\nametag{$^{1,2}$}} \orcid{1111-2222-3333-4444} \Email{serge.vasylechko@childrens.harvard.edu}\\
\addr $^{1}$ Quantitative Intelligent Imaging Laboratory, Department of Radiology, Boston Children's Hospital, Longwood Avenue, Boston, MA 02115 \\
\addr $^{2}$ Harvard Medical School, Longwood Avenue, Boston, MA 02115 \AND
\Name{Andy Tsai\nametag{$^{1,2}$}} \Email{andy.tsai@childrens.harvard.edu}\\
\Name{Onur Afacan\nametag{$^{1,2}$}} \Email{onur.afacan@childrens.harvard.edu}\\
\Name{Sila Kurugol\nametag{$^{1,2}$}} \Email{sila.kurugol@childrens.harvard.edu}
}

\begin{document}

% \maketitle

\begin{abstract}
Deep learning models for pediatric fracture detection face significant challenges including limited high-quality annotated data, domain shift between X-ray machines, and class imbalances. Conventional supervised annotation methods are time-consuming and expensive, making it difficult to develop robust detection systems that perform well across all fracture types and variations. To address these challenges, we developed an automated, cost-free method that combines large language models (LLMs), vision-language models (VLMs), and anatomical knowledge to extract structured information from radiology reports and generate high-quality pseudo-labels for training improved fracture detection models. 
%Using bone co-locations and these ground truth labels derived from radiology reports we are able to finetune and significantly improve performance over a pretrained YOLO backbone  

We present a multi-stage automated pipeline that integrates multiple modalities for enhanced fracture detection. First, we extract fracture type, bone location, and severity from radiology reports using LLMs with chain-of-thought reasoning and few-shot learning. Second, we train an AP view classifier and bone detection network to localize four anatomical regions (radius, ulna, ulnar styloid, scaphoid), providing anatomical context. Third, we co-localize pretrained fracture predictions with bone regions and validate against report findings to automatically generate true positive, false positive, and false negative pseudo-labels. Fourth, we employ VLMs as an additional verification layer to confirm anatomical correctness. Finally, we finetune with focal loss to learn effectively from the curated pseudo-labeled dataset.

Across a dataset of 71,714 pediatric wrist examinations from Boston Children's Hospital, our approach achieves substantial improvements in detection performance. On a stratified test set of 400 images (100 per bone type), our model achieves 76.9\% macro-averaged recall and 86.2\% precision compared to 58.7\% and 72.0\% for the pretrained baseline. The system demonstrates consistent improvements across all fracture types, including both common fractures (distal radius) and clinically critical rare fractures (scaphoid: 15.1\% → 67.4\% recall improvement). This work demonstrates that multi-modal pseudo-labeling combining LLMs, VLMs, and anatomical knowledge can significantly enhance fracture detection without requiring extensive manual annotation, providing a scalable approach for medical object detection tasks.
\end{abstract}

\begin{keywords}
LLM, object detection, limited data, weak labels
\end{keywords}

\section{Introduction}

\subsection{Clinical Motivation: Pediatric Wrist Fracture Detection}

Fractures represent one of the most common injuries in children, accounting for approximately 25\% of all pediatric injuries \cite{butler2023epidemiology}. Accurate and timely diagnosis is crucial, as delays can lead to significant complications including improper healing and long-term functional impairments \cite{tadepalli2022nonaccidental}. However, pediatric fractures pose unique diagnostic challenges due to developing anatomy, with X-rays remaining a source of common diagnostic errors in emergency departments.

Deep learning has shown promise for automated fracture detection, yet clinical deployment is hindered by the need for extensive, high-quality annotated datasets. Manual annotation by radiologists is time-consuming, expensive, and suffers from inter-observer variability. Moreover, wrist fracture data follows an imbalanced distribution where distal radius fractures dominate (53.5\% of cases), while clinically critical fractures such as scaphoid comprise only 3.8\% of presentations (Table \ref{tab:fracture_simple}). This poses challenges for AI systems, which may perform well on common fractures but fail on rare yet important cases.

Scaphoid fractures, though less frequent, require particular attention. If left untreated, they can develop avascular necrosis due to disrupted blood supply, leading to bone death, chronic pain, and long-term disability \cite{scaphoid_necrosis}. Similarly, ulnar styloid fractures (12.9\%) can affect the growth plate in pediatric patients, potentially causing growth disturbances if missed \cite{marsh2007fracture}. A robust detection system must perform reliably across all fracture types, including both common and rare presentations.

\subsection{Challenges in Medical Object Detection}

The diagnostic performance of deep learning methods in fracture detection remains constrained by several fundamental challenges. First, insufficient expertly annotated datasets limit model training, particularly for rare fracture patterns. Second, inter-observer variability in manual annotation creates inconsistent training signals. Third, class distribution imbalances mean that models trained on large public datasets (e.g., GRAZPEDWRI-DX \cite{nagy2022pediatric}) may demonstrate reasonable overall performance but fail on underrepresented classes.

Traditional annotation methods requiring bounding boxes are prohibitively expensive for large-scale applications. At current rates, manually annotating thousands of images requires extensive radiologist time. This problem is exacerbated when seeking to label rare fractures—finding 100 scaphoid fractures at 3.8\% prevalence requires reviewing approximately 2,600 examinations, each with multiple views.

Recent approaches have explored pseudo-labeling to reduce annotation burden \cite{keuth2024sam}, leveraging models like Segment Anything Model (SAM) for weakly supervised label generation. However, most methods do not explicitly validate pseudo-labels against clinical documentation or incorporate anatomical knowledge to improve label quality. The emergence of large language models (LLMs) and vision-language models (VLMs) presents new opportunities to extract structured information from radiology reports and validate detection predictions.

\subsection{Our Approach: Multi-Modal Pseudo-Labeling}

We present a multi-stage automated pipeline that combines LLMs, VLMs, and anatomical knowledge to generate high-quality pseudo-labels for fracture detection without extensive manual annotation. Our key insight is that multiple complementary information sources—radiology reports, anatomical structure, and visual confirmation—can be integrated to create reliable training labels automatically.

The pipeline operates in five stages:

\begin{enumerate}
\item \textbf{LLM-based report extraction}: Extract structured fracture information (type, location, severity) from clinical reports using chain-of-thought reasoning and few-shot learning
\item \textbf{Anatomical filtering}: Train an AP view classifier to select images with clearly visible bone structures
\item \textbf{Bone detection}: Deploy a bone detection network to localize four anatomical regions (radius, ulna, ulnar styloid, scaphoid)
\item \textbf{Multi-modal validation}: Co-localize pretrained fracture predictions with bone regions, validate against report findings, and employ VLMs to confirm anatomical correctness
\item \textbf{Targeted training}: Generate true positive, false positive, and false negative pseudo-labels, then finetune with focal loss to emphasize challenging cases
\end{enumerate}

This approach enables systematic improvement across all fracture types by leveraging the natural information present in clinical workflows—radiology reports and anatomical structure—without requiring exhaustive manual labeling.

\subsection{Contributions}

This work makes the following contributions. We develop an automated multi-modal pseudo-labeling pipeline that integrates LLMs for report extraction, anatomical bone detection for spatial validation, and VLMs for visual confirmation.  We show that the system achieves strong performance on both common fractures (distal radius) and rare but critical fractures (scaphoid recall: 15.1\% → 67.4\%; ulnar styloid: improved metrics). Our code and data processing pipeline will be made available at www.github.com/sergeicu/yolostructured.


\section{Method}

\subsection{Pipeline Overview}

Figure \ref{fig:pipeline} illustrates our complete multi-stage pipeline integrating LLMs, anatomical knowledge, and VLMs for automated pseudo-labeling. The pipeline consists of five principal stages: (1) LLM-based extraction of structured fracture information from radiology reports using chain-of-thought reasoning and few-shot learning, (2) AP view classification to filter anatomically informative images, (3) bone detection network to localize four anatomical structures providing spatial context, (4) multi-modal validation through co-localization of fracture predictions with bone regions, verification against LLM-extracted report data, and VLM-based confirmation of anatomical correctness to generate high-quality pseudo-labels (TP/FP/FN), and (5) targeted finetuning with focal loss on the automatically curated dataset. This integrated approach leverages complementary information sources to generate reliable training labels without manual annotation. Each component is described in detail in the following subsections.

\begin{figure}[!ht]
\floatconts
  {fig:pipeline}
  {\caption{\textbf{Multi-stage pipeline for multi-modal pseudo-labeling.} The pipeline integrates multiple information sources for automated label generation. (Top) Radiology reports undergo anonymization and LLM-based extraction to obtain structured fracture information (bone type, location, severity). (Middle) X-ray images are filtered by an AP view classifier. A bone detection network localizes four anatomical regions, while a pretrained fracture detector generates initial predictions. (Bottom) Multi-modal validation combines co-localization (matching fracture predictions to bone regions via IoU overlap), LLM-report validation, and VLM-based visual confirmation to classify predictions as true positive (TP), false positive (FP), or false negative (FN). The automatically curated dataset with high-quality pseudo-labels undergoes focal loss-based finetuning for enhanced performance across all fracture types.}}
  {\includegraphics[width=1.0\linewidth]{ims/fig22.png}}
\end{figure}


\subsection{Data Collection and Preprocessing}

An extensive dataset of 71,714 clinical pediatric wrist X-ray examinations was retrospectively acquired from Boston Children's Hospital radiology department under approved IRB protocol. The dataset comprised examinations spanning January 2001 through August 2024, each consisting of multiple image projections following standard radiological protocols (anteroposterior, lateral, oblique, and scaphoid-specific views when clinically indicated). All examinations included corresponding clinical radiology reports.

\textbf{Domain Shift Between Training and Deployment Data:} A critical challenge in applying pretrained fracture detection models to new clinical populations is domain shift—differences in imaging protocols, patient demographics, and equipment characteristics between the original training dataset and the deployment environment. We quantified this phenomenon by extracting deep feature representations from the pretrained YOLO v7 model (trained on GRAZPEDWRI-DX \cite{nagy2022pediatric}) and projecting them using UMAP dimensionality reduction. Features from 30 randomly selected images from both the GRAZ dataset and our Boston Children's Hospital (BCH) dataset revealed substantial separation in feature space, indicating significant domain shift (Figure \ref{fig:domain_shift}). This distributional mismatch directly impacts model performance: the pretrained model achieved only 76\% mAP on our clinical data despite strong performance on its original training distribution. This finding motivates our pseudo-labeling approach—by selectively finetuning on institution-specific data using automated labels, we adapt the model to the target distribution without requiring extensive manual annotation.

\begin{figure}[htbp]
\floatconts
  {fig:domain_shift}
  {\caption{\textbf{Domain shift between GRAZ and BCH datasets.} (A) Example showing pretrained YOLO correctly detecting distal radius Salter-Harris II fracture but missing distal ulna fracture and incorrectly marking metacarpals. (B) UMAP projection of YOLO feature representations (layer 104) from 30 images shows clear separation between GRAZ training data (blue) and BCH clinical data (red), quantifying substantial domain shift that limits pretrained model performance to 76\% mAP on target distribution.}}
  {\includegraphics[width=1.0\linewidth]{ims/fig1.png}}
\end{figure}

A multi-stage anonymization protocol was implemented to ensure compliance with patient privacy regulations. First, a rule-based system removed explicit identifiers including dates, ages, names, gender, medical record numbers, and examination metadata. DICOM headers were stripped of patient-specific information. Subsequently, the LLMguard framework \cite{llmguard} was applied as a secondary verification layer to detect any remaining temporal or nominal identifiers. Prior diagnosis sections were removed while preserving impression and findings sections for LLM extraction.

Table \ref{tab:fracture_simple} presents the class distribution across bone types extracted from radiology reports. The severe imbalance is evident: distal radius fractures account for 53.5\% of cases, while clinically critical scaphoid fractures represent only 3.8\%. This 14-fold difference in prevalence creates the fundamental challenge that motivates our bone-aware pseudo-labeling approach.

\begin{table}[htbp]
\centering
\caption{\textbf{Severe class imbalance in pediatric wrist fracture data.} Distribution of fracture types extracted from 71,714 radiology reports shows distal radius dominance (53.5\%) while rare but critical scaphoid fractures account for only 3.8\% of cases. This imbalance directly translates to model performance disparities without targeted intervention.}
\label{tab:fracture_simple}
\begin{tabular}{|l|l|r|c|}
\hline    
\textbf{Dataset Type} & \textbf{Class} & \textbf{Count} & \textbf{Percentage} \\
\hline
\multirow{5}{*}{Anatomical Regions} & Distal Radius & 37,676 & 53.5\% \\
& Distal Ulna & 12,589 & 17.9\% \\
& Ulnar Styloid & 9,122 & 12.9\% \\
& Scaphoid & 2,709 & 3.8\% \\
& No Fracture & 8,347 & 11.8\% \\
\hline
\end{tabular}
\end{table}

\begin{comment}
\begin{table}[htbp]
\centering
\caption{Fracture Dataset Class Distribution}
\label{tab:fracture_simple}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Dataset Type} & \textbf{Class} & \textbf{Count} & \textbf{Percentage} \\
\hline
Anatomical Regions & distal\_radius\_shaft & 7,299 & 70.1\% \\
& distal\_ulna\_shaft & 2,871 & 27.6\% \\
& ulnar\_styloid & 1,685 & 16.2\% \\
& scaphoid & 418 & 4.0\% \\
& no\_fracture & 2,750 & 26.4\% \\
\hline
Classification & transverse & 2,307 & 26.0\% \\
& salter\_harris\_II & 2,263 & 25.5\% \\
& buckle & 1,244 & 14.0\% \\
& comminuted & 244 & 2.7\% \\
& avulsion & 99 & 1.1\% \\
& no\_fracture & 2,750 & 31.0\% \\
\hline
Healing & healing & 6,008 & 64.9\% \\
& acute & 1,325 & 14.3\% \\
& healed & 255 & 2.8\% \\
& nonunion & 129 & 1.4\% \\
& no\_fracture & 2,750 & 29.7\% \\
\hline
Alignment & well\_aligned & 6,712 & 67.4\% \\
& acceptable\_alignment & 2,171 & 21.8\% \\
& poor\_alignment & 327 & 3.3\% \\
& no\_fracture & 2,750 & 27.6\% \\
\hline
\end{tabular}
\end{table}

\end{comment}]
\begin{comment}
    

\begin{table}[htbp]
\centering
\caption{Complete Dataset Distribution - All Available Categories}
\label{tab:fracture_complete}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Category} & \textbf{Class} & \textbf{Count} & \textbf{Percentage} \\
\hline
Anatomical Regions & distal\_radius\_shaft & 7,299 & 47.7\% \\
& distal\_ulna\_shaft & 2,871 & 18.8\% \\
& not\_applicable & 2,750 & 18.0\% \\
& ulnar\_styloid & 1,685 & 11.0\% \\
& scaphoid & 418 & 2.7\% \\
& unknown & 137 & 0.9\% \\
& other & 98 & 0.6\% \\
& radial\_styloid & 21 & 0.1\% \\
& radius\_other & 21 & 0.1\% \\
& proximal\_radius & 4 & 0.0\% \\
& ulna\_other & 2 & 0.0\% \\
& proximal\_ulna & 1 & 0.0\% \\
\hline
Classification & unknown & 5,976 & 39.0\% \\
& not\_applicable & 2,750 & 18.0\% \\
& transverse & 2,307 & 15.1\% \\
& salter\_harris\_II & 2,263 & 14.8\% \\
& buckle & 1,244 & 8.1\% \\
& comminuted & 244 & 1.6\% \\
& oblique & 226 & 1.5\% \\
& avulsion & 99 & 0.6\% \\
& other & 98 & 0.6\% \\
& salter\_harris\_IV & 39 & 0.3\% \\
& greenstick & 36 & 0.2\% \\
& salter\_harris\_I & 25 & 0.2\% \\
\hline
Healing & healing & 6,008 & 39.3\% \\
& unknown & 4,840 & 31.6\% \\
& not\_applicable & 2,750 & 18.0\% \\
& acute & 1,325 & 8.7\% \\
& healed & 255 & 1.7\% \\
& nonunion & 129 & 0.8\% \\
\hline
Alignment & well\_aligned & 6,712 & 43.8\% \\
& unknown & 3,347 & 21.9\% \\
& not\_applicable & 2,750 & 18.0\% \\
& acceptable\_alignment & 2,171 & 14.2\% \\
& poor\_alignment & 327 & 2.1\% \\
\hline
Immobilization & unknown & 7,588 & 49.6\% \\
& cast & 4,959 & 32.4\% \\
& not\_applicable & 2,750 & 18.0\% \\
& other & 10 & 0.1\% \\
\hline
Fracture Count & 0 & 2,750 & 18.0\% \\
& 1 & 3,419 & 22.3\% \\
& 2 & 8,180 & 53.4\% \\
& 3 & 813 & 5.3\% \\
& 4 & 140 & 0.9\% \\
& 5 & 5 & 0.0\% \\
\hline
Has Fracture & True & 12,557 & 82.0\% \\
& False & 2,750 & 18.0\% \\
\hline
\end{tabular}
\end{table}
\end{comment}


% Add these packages to your preamble if not already included
% \usepackage{graphicx}
% \usepackage{float}

\begin{comment}
% Figure 6: Distribution of Fracture Counts
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{ims/num_fractures.png}
    \caption{Distribution of fracture counts per case in the dataset. This distribution indicates that most clinical cases in the dataset involve multiple fractures, which is consistent with the complex nature of pediatric wrist injuries.}
    \label{fig:fracture_count_distribution}
\end{figure}

% Figure 1: Alignment Category Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_alignment.png}
    \caption{Cross-tabulation heatmaps showing the distribution of alignment categories against other dataset features. The figure displays six heatmaps examining alignment category relationships with: (top row) anatomical region, healing category, and classification category; (bottom row) fracture count, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The alignment categories include well\_aligned, acceptable\_alignment, poor\_alignment, unknown, and not\_applicable.}
    \label{fig:alignment_crosstab}
\end{figure}

% Figure 2: Anatomical Region Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_anatomical_region.png}
    \caption{Cross-tabulation heatmaps showing the distribution of anatomical regions against other dataset features. The figure displays six heatmaps examining anatomical region relationships with: (top row) alignment category, healing category, and classification category; (bottom row) fracture count, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The anatomical regions include distal\_radius\_shaft, distal\_ulna\_shaft, ulnar\_styloid, scaphoid, and several other less frequent regions.}
    \label{fig:anatomical_crosstab}
\end{figure}

% Figure 3: Classification Category Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_classification.png}
    \caption{Cross-tabulation heatmaps showing the distribution of fracture classification categories against other dataset features. The figure displays six heatmaps examining classification category relationships with: (top row) anatomical region, healing category, and alignment category; (bottom row) fracture count, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The classification categories include transverse, salter\_harris\_II, buckle, comminuted, avulsion, and several other fracture types, with a large proportion classified as unknown.}
    \label{fig:classification_crosstab}
\end{figure}

% Figure 4: Fracture Count Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_fracture_count.png}
    \caption{Cross-tabulation heatmaps showing the distribution of fracture counts (0-5 fractures per image) against other dataset features. The figure displays six heatmaps examining fracture count relationships with: (top row) anatomical region, healing category, and classification category; (bottom row) alignment category, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The majority of cases show 2 fractures per image, with decreasing frequency for higher fracture counts.}
    \label{fig:fracture_count_crosstab}
\end{figure}

% Figure 5: Healing Category Cross-Tabulations
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ims/tabulations_healing.png}
    \caption{Cross-tabulation heatmaps showing the distribution of healing categories against other dataset features. The figure displays six heatmaps examining healing category relationships with: (top row) anatomical region, alignment category, and classification category; (bottom row) fracture count, immobilization type, and imaging side. Color intensity represents frequency counts, with darker colors indicating higher frequencies. The healing categories include healing, acute, healed, nonunion, with a substantial proportion classified as unknown or not\_applicable.}
    \label{fig:healing_crosstab}
\end{figure}

\end{comment}

\subsection{LLM-Based Report Information Extraction}

A structured natural language processing pipeline was developed to extract fracture bone type information from radiological reports. This component serves as the ground truth for validating pseudo-labels generated by the object detection pipeline.

We utilized the MedGemma 27B model for extraction due to its demonstrated performance on medical domain tasks. The extraction focused specifically on identifying which bones contained fractures (radius, ulna, ulnar styloid, scaphoid) rather than detailed fracture characterization, as anatomical localization is sufficient for co-localization validation. 


\subsubsection{Schema Validation}
The following steps were implemented to ensure that the responses from the LLM adhered to a strict predictable structured response pattern:
1. Report Pre-processing: The initial step involved removing non-standard characters and normalizing medical abbreviations according to RADLEX terminology standards \cite{langlotz2006radlex}. 
2. Schema Design: A schema was created to capture the hierarchical relationships between anatomical structures, fracture characteristics, and associated findings.
3. Core Extraction: The main extraction process utilized explicitly defined JSON structures passed to the LLM system, with response formatting managed through prefilled templates.
4. Type Validation: Type validation was enforced using Pydantic \cite{pydantic} to ensure structured response patterns.

\subsubsection{Prompt Engineering and Safeguards}
To minimize hallucination risks, we implemented: (1) Chain-of-Thought reasoning \cite{nori2023can} aligned with radiological protocols, (2) Curated few-shot examples for fracture patterns, (3) Strict "explicitly stated observations" scope, and (4) Comprehensive confidence scoring and audit trails. Manual verification of 250 cases revealed a hallucination rate of 0.98\%.

\subsection{AP View Classification}

Pediatric wrist examinations typically include multiple radiographic projections (anteroposterior, lateral, oblique). However, bone boundaries are only clearly distinguishable in AP views---lateral and oblique views exhibit substantial bone overlap that confounds both bone detection and fracture localization. To select anatomically informative images, we trained a binary CNN classifier to distinguish AP views from other projections.

\textbf{Architecture and Training:} We employed a ResNet-18 architecture \cite{resnet} pretrained on ImageNet and finetuned on manually labeled wrist X-rays. The training dataset comprised 850 AP-view images and 650 non-AP images (lateral, oblique, scaphoid-specific), annotated by a radiologist based on DICOM view tags and visual inspection. Images were resized to 224×224 pixels with standard normalization. Training used the Adam optimizer with learning rate 1e-4, batch size 32, and cross-entropy loss over 25 epochs.

\textbf{Performance:} The AP classifier achieved 96.2\% accuracy on a held-out test set of 200 images, with precision 95.8\% and recall 97.1\% for the AP class. This high accuracy ensures that downstream bone detection operates on anatomically clear views.

\subsection{Bone Detection Network}

The bone detection network localizes four anatomical structures---distal radius, distal ulna, ulnar styloid, and scaphoid---providing the spatial context necessary for fracture-bone co-localization. Critically, bone detection is substantially more tractable than fracture detection for three reasons: 1. Consistent presence: All four bones appear in every wrist AP radiograph (barring amputation), eliminating the challenge of rare positive examples. 2. Structural invariance: Bone shapes and relative positions exhibit low inter-patient variability compared to fracture patterns. 3. Clear boundaries: Bone cortex produces high-contrast edges in X-rays, unlike subtle fracture lines.

\textbf{Manual Annotation:} A board-certified radiologist annotated 300 AP-view images with bounding boxes for all four bones using a custom Flask-based labeling interface. Each annotation session presented all images from a single examination alongside the anonymized report for reference. The annotation process required approximately 12 hours, yielding 1,200 bone bounding boxes (300 images × 4 bones).

\textbf{Architecture and Training:} We trained a YOLOv7 model \cite{wang2023yolov7} from scratch using the standard architecture with 5-stage backbone (32 to 1024 channels) and three-scale detection head. Nine anchor boxes were distributed across scales corresponding to the range of bone sizes. Training employed SGD optimization with learning rate 0.01, momentum 0.937, weight decay 0.0005, and batch size 16. A cosine annealing schedule with 3-epoch warmup was used over 100 epochs. Augmentation included rotation (±10°), scaling (±10\%), and brightness jitter (HSV ±0.15), while mosaic and mixup were disabled to preserve anatomical coherence. Images were resized to 640×640 pixels.

\textbf{Performance:} Table \ref{tab:bone_detection} presents per-class mAP@0.5 for bone detection. The model achieved high accuracy across all four bone types, with particularly strong performance on radius and ulna due to their larger size and clearer boundaries.

% \begin{table}[htbp]
% \centering
% \caption{\textbf{Bone detection performance by anatomical class.} YOLOv7 bone detector trained on 300 manually annotated images achieves high mAP@0.5 across all four bone types. Strong performance enables reliable downstream co-localization with fracture predictions.}
% \label{tab:bone_detection}
% \begin{tabular}{|l|c|c|c|}
% \hline
% \textbf{Bone Type} & \textbf{mAP@0.5} & \textbf{Precision} & \textbf{Recall} \\
% \hline
% Distal Radius & [0.XX] & [0.XX] & [0.XX] \\
% Distal Ulna & [0.XX] & [0.XX] & [0.XX] \\
% Ulnar Styloid & [0.XX] & [0.XX] & [0.XX] \\
% Scaphoid & [0.XX] & [0.XX] & [0.XX] \\
% \hline
% \textbf{Mean} & [0.XX] & [0.XX] & [0.XX] \\
% \hline
% \end{tabular}
% \end{table}

\subsection{Bone-Fracture Co-Localization and Pseudo-Label Generation}

This component forms the core of our bone-aware pseudo-labeling approach. Given an AP-view image, we: (1) detect bone regions using the bone YOLO, (2) detect fractures using the pretrained Graz YOLO \cite{nagy2022pediatric, ciri2023bonefracture}, (3) assign each fracture prediction to a bone via spatial overlap, and (4) validate the bone-fracture pair against LLM-extracted report data to classify as true positive (TP), false positive (FP), or false negative (FN).

\textbf{Spatial Assignment via IoU:} For each fracture bounding box $b_f$ predicted by the Graz model, we compute intersection-over-union (IoU) with all bone bounding boxes $\{b_1, b_2, b_3, b_4\}$:
\begin{equation}
\text{IoU}(b_f, b_i) = \frac{\text{Area}(b_f \cap b_i)}{\text{Area}(b_f \cup b_i) }
\end{equation}
The fracture is assigned to the bone with maximum IoU if $\max_i \text{IoU}(b_f, b_i) > \tau$, where threshold $\tau = 0.3$ was empirically selected to balance sensitivity and specificity. Fractures with no bone overlap above threshold are discarded as anatomically implausible.

\textbf{Report-Based Validation:} Each image is associated with an examination-level radiology report specifying which bones contain fractures (e.g., "distal radius fracture" indicates fracture in the radius bone). The LLM extraction provides a set of fractured bones $F_{\text{report}} \subset \{\text{radius, ulna, styloid, scaphoid}\}$ for each examination. Given a fracture prediction assigned to bone $b_i$, we classify it as:
\begin{itemize}
\item \textbf{True Positive (TP)}: $b_i \in F_{\text{report}}$ (model detected fracture in correct bone)
\item \textbf{False Positive (FP)}: $b_i \notin F_{\text{report}}$ (model hallucinated fracture in wrong bone)
\item \textbf{False Negative (FN)}: $b_i \in F_{\text{report}}$ but no prediction (model missed fracture)
\end{itemize}

\textbf{Handling Multi-View Examinations:} Radiology reports describe findings across all views but do not specify which view displays each fracture. We encode anatomical knowledge to resolve ambiguities: (1) lateral views cannot distinguish radius from ulna fractures, so only AP view radius/ulna fractures are validated; (2) ulnar styloid fractures are poorly visualized in lateral views, so only AP instances are considered; (3) scaphoid fractures may require dedicated scaphoid-view projections. By filtering to AP views and applying these heuristics, we ensure spatial consistency between predictions and report findings.

\subsection{Vision-Language Model Verification}

While LLM-extracted report data provides semantic validation of fracture locations, and bone detection provides spatial context, we employ vision-language models (VLMs) as an additional verification layer to confirm the anatomical correctness of detected regions. Although VLMs are not specifically trained for medical fracture detection tasks, recent work has demonstrated their surprising effectiveness in understanding general anatomical structures in medical images \cite{almohamad2025vlm}.

\textbf{Visual Confirmation Protocol:} For each YOLO-detected fracture candidate that passes initial co-localization validation, we encode the X-ray image along with the highlighted bounding box region into base64 format. The VLM is then queried to verify whether the marked anatomical area is plausible for the specified bone type. This provides a visual "guardrail" that helps identify cases where YOLO predictions may be spatially misaligned despite passing IoU thresholds—for example, detecting metacarpal bones instead of actual wrist fractures.

\textbf{Query Design:} The VLM receives the annotated image and a structured query: "Is the highlighted bounding box region anatomically consistent with a [bone\_type] fracture?" The VLM response is parsed to extract binary confirmation (yes/no) along with confidence scores. Predictions receiving low confidence scores or negative confirmation are flagged for review or exclusion from the pseudo-labeled training set.

This multi-modal verification strategy (LLM report validation + spatial co-localization + VLM visual confirmation) significantly improves the reliability of automatically generated pseudo-labels, reducing false positives that could otherwise corrupt model training. The VLM layer proved particularly valuable for catching edge cases where bone detection or fracture detection produced anatomically implausible results despite satisfying numerical criteria.

\subsection{Automated Data Curation and Balanced Sampling}

The co-localization process generates a dataset with explicit TP/FP/FN labels for each bone type. We leverage these labels to create a curated training set that systematically addresses class imbalance:

\textbf{True Positives:} TP images are directly included in the training set with their original bounding boxes. These represent correctly detected fractures and reinforce accurate model behavior.

\textbf{False Positives:} FP images are included with the erroneous bounding box removed, converting them into hard negative examples. This teaches the model to avoid hallucinating fractures in specific anatomical regions (e.g., not predicting radius fractures when only ulna is fractured).

\textbf{False Negatives:} FN cases represent missed fractures, which are particularly valuable for rare classes. Due to resource constraints, we manually annotate a targeted subset of FN cases prioritized by bone type rarity (scaphoid > ulnar styloid > ulna > radius). A radiologist provided bounding boxes for 150 FN cases using the Flask-based interface, focusing on 75\% of scaphoid FNs and 50\% of ulnar styloid FNs.

Table \ref{tab:pseudolabel_stats} summarizes the dataset statistics after automated pseudo-labeling.

% \begin{table}[htbp]
% \centering
% \caption{\textbf{Pseudo-label generation statistics by bone type.} Automated co-localization produces class-specific TP/FP/FN counts. Manual annotation effort focuses on rare class FNs (scaphoid, ulnar styloid), generating [XXXX] total training labels with only [YYY] manual annotations required.}
% \label{tab:pseudolabel_stats}
% \begin{tabular}{|l|r|r|r|r|r|}
% \hline
% \textbf{Bone Type} & \textbf{Total} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{FN Manual} \\
% & \textbf{Images} & & & \textbf{Detected} & \textbf{Labeled} \\
% \hline
% Distal Radius & [XXXX] & [XXXX] & [XXX] & [XXX] & [XX] \\
% Distal Ulna & [XXXX] & [XXXX] & [XXX] & [XXX] & [XX] \\
% Ulnar Styloid & [XXXX] & [XXXX] & [XXX] & [XXX] & [XXX] \\
% Scaphoid & [XXXX] & [XXX] & [XXX] & [XXX] & [XXX] \\
% \hline
% \textbf{Total} & [XXXXX] & [XXXXX] & [XXXX] & [XXXX] & [YYY] \\
% \hline
% \end{tabular}
% \end{table}

\subsection{Focal Loss-Based Finetuning}

The final stage employs focal loss \cite{focal_loss} to explicitly down-weight easy examples (common, well-detected fractures) and focus learning on hard examples (rare, challenging fractures). Focal loss modifies the standard cross-entropy loss by adding a modulating factor $(1-p_t)^\gamma$:
\begin{equation}
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}
where $p_t$ is the predicted probability of the correct class, $\alpha_t$ is a class-weighting factor, and $\gamma$ is the focusing parameter. We set $\gamma = 2.0$ following \cite{focal_loss} and assign class weights $\alpha$ inversely proportional to class frequency to provide additional emphasis on rare classes.

Algorithm \ref{alg:focal_loss_training} presents the complete training procedure with focal loss applied to the curated pseudo-labeled dataset.

\begin{algorithm}[htbp]
\caption{Focal Loss Training on Pseudo-Labeled Dataset}
\label{alg:focal_loss_training}
\KwIn{Curated dataset $\mathcal{D} = \{(x_i, y_i, c_i)\}$ with images $x_i$, bounding boxes $y_i$, and bone class labels $c_i$}
\KwIn{Pretrained YOLO model $f_\theta$ (GRAZ weights)}
\KwIn{Focal loss parameters: $\gamma = 2.0$, class weights $\{\alpha_c\}$ inversely proportional to class frequency}
\KwOut{Finetuned model $f_{\theta^*}$ optimized for rare fracture detection}

\BlankLine
\textbf{Initialize:} Load pretrained weights $\theta \leftarrow \theta_{\text{GRAZ}}$\;
Set optimizer: SGD with lr=0.001, momentum=0.937, weight decay=0.0001\;
Set learning rate schedule: One-cycle cosine annealing with 3-epoch warmup\;
Set batch size $B = 16$, resolution $1280 \times 1280$, epochs $E = 50$\;

\BlankLine
\For{epoch $e = 1$ to $E$}{
    Shuffle dataset $\mathcal{D}$\;
    \For{each mini-batch $\mathcal{B} \subset \mathcal{D}$ of size $B$}{
        \tcp{Forward pass with multi-scale augmentation}
        Apply augmentation: rotation $\pm 10°$, scale $\pm 10\%$, HSV jitter $\pm 0.15$\;
        Predictions $\hat{y}, \hat{c}, \hat{s} \leftarrow f_\theta(x)$ for boxes, classes, objectness\;
        
        \BlankLine
        \tcp{Compute focal loss for classification}
        $\mathcal{L}_{\text{focal}} \leftarrow 0$\;
        \ForEach{prediction $j$ with class $c_j$ and probability $p_j$}{
            $\mathcal{L}_{\text{focal}} \leftarrow \mathcal{L}_{\text{focal}} - \alpha_{c_j} (1 - p_j)^\gamma \log(p_j)$\;
        }
        
        \BlankLine
        \tcp{Compute bounding box and objectness losses}
        $\mathcal{L}_{\text{IoU}} \leftarrow$ IoU regression loss for box refinement\;
        $\mathcal{L}_{\text{obj}} \leftarrow$ Binary cross-entropy for foreground/background\;
        
        \BlankLine
        \tcp{Combined loss with tuned weights for X-ray detection}
        $\mathcal{L}_{\text{total}} \leftarrow 0.1 \cdot \mathcal{L}_{\text{focal}} + 0.05 \cdot \mathcal{L}_{\text{IoU}} + 0.8 \cdot \mathcal{L}_{\text{obj}}$\;
        
        \BlankLine
        \tcp{Backpropagation and optimization}
        Compute gradients $\nabla_\theta \mathcal{L}_{\text{total}}$\;
        Update parameters $\theta$ via SGD step\;
    }
    Update learning rate via cosine schedule\;
    Evaluate on validation set\;
}
\Return{Optimized model $f_{\theta^*}$}\;
\end{algorithm}

\textbf{Training Configuration:} We finetune the Graz-pretrained YOLOv7 model using the curated dataset with focal loss applied to classification predictions. Training employs SGD optimization with learning rate 0.001, momentum 0.937, weight decay 0.0001, and batch size 16. A one-cycle cosine annealing schedule with 3-epoch warmup runs for 50 epochs. Multi-scale training uses base resolution 1280×1280. Augmentation matches the bone detection configuration. The IoU threshold for positive assignment is set to 0.15 to accommodate imprecise fracture boundaries.

\textbf{Loss Function Components:} The total loss combines focal classification loss, IoU regression loss for bounding box refinement, and objectness loss for foreground-background separation:
\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_{\text{cls}} \mathcal{L}_{\text{focal}} + \lambda_{\text{box}} \mathcal{L}_{\text{IoU}} + \lambda_{\text{obj}} \mathcal{L}_{\text{objectness}}
\end{equation}
with weights $\lambda_{\text{cls}} = 0.1, \lambda_{\text{box}} = 0.05, \lambda_{\text{obj}} = 0.8$ tuned for X-ray characteristics.

\subsection{Evaluation Protocol}

We evaluate model performance on a held-out test set of 400 expert-annotated images, stratified with 100 images per bone type (100 scaphoid, 100 distal radius, 100 distal ulna, 100 ulnar styloid). A board-certified radiologist provided ground truth bounding boxes and class labels for all fractures. This stratified sampling ensures adequate representation of rare fracture types for robust performance evaluation. The remaining 71,314 examination images were split 90:10 into training and validation sets, with the validation set used for hyperparameter tuning and early stopping. Metrics are computed both overall (macro-averaged across bone types) and per bone class to explicitly measure improvements on rare fractures. 

 



\begin{comment}
% The final dataset consisted of grayscale PNG images, and corresponding radiology reports in the JSON format. 
\begin{table}[h]
\centering
\caption{Distribution of Wrist Fractures and Their Characteristics}
\begin{tabular}{lrr}
\hline
\textbf{Characteristic} & \textbf{Count} & \textbf{Percentage (\%)} \\
\hline
\multicolumn{3}{l}{\textit{Anatomical Location}} \\
Radius & 934 & 86.2 \\
Scaphoid & 94 & 8.7 \\
Ulna & 41 & 3.8 \\
Other & 14 & 1.3 \\
\hline
\multicolumn{3}{l}{\textit{Fracture Region}} \\
Distal & 717 & 66.2 \\
Metaphysis & 267 & 24.7 \\
Other & 99 & 9.1 \\
\hline
\multicolumn{3}{l}{\textit{Fracture Pattern}} \\
Transverse & 230 & 21.2 \\
Torus & 149 & 13.8 \\
Simple & 74 & 6.8 \\
Oblique & 26 & 2.4 \\
Comminuted & 23 & 2.1 \\
\hline
\multicolumn{3}{l}{\textit{Healing Stage}} \\
Healing & 543 & 50.1 \\
Acute & 261 & 24.1 \\
Early Healing & 101 & 9.3 \\
Other & 178 & 16.5 \\
\hline
\end{tabular}
\label{tab:fracture_distribution}
\end{table}

\end{comment}



\section{Results}

\subsection{Pretrained Model Performance Reveals Severe Class Imbalance}
\label{sec:baseline_results}

Table \ref{tab:baseline_performance} quantifies the class-specific performance of the Graz-pretrained YOLOv7 model on our stratified test set of 400 images (100 per bone type, each containing an isolated fracture of that bone). The results starkly demonstrate the severity of the class imbalance problem: while the model achieves reasonable performance on common distal radius fractures (80.5\% recall, 87.7\% precision), performance degrades substantially on ulnar styloid (59.6\% recall) and collapses catastrophically on rare scaphoid fractures (15.1\% recall, 33.3\% precision). This 5.3× performance gap in recall between the most common (radius) and rarest (scaphoid) class validates our core hypothesis that pretrained models fail on rare but clinically critical classes, despite adequate overall performance metrics.

\begin{table}[htbp]
\centering
\caption{\textbf{Pretrained model fails on rare fracture classes.} Graz-pretrained YOLOv7 performance on our test set (100 images per bone type, 400 total) shows severe degradation on rare bones. While distal radius (53.5\% training prevalence) achieves 80.5\% recall and 87.7\% precision, scaphoid (3.8\% prevalence) suffers catastrophic failure with only 15.1\% recall and 33.3\% precision. This 5.3× performance gap in recall motivates our bone-aware pseudo-labeling approach.}
\label{tab:baseline_performance}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Bone Type} & \textbf{Training} & \textbf{Precision} & \textbf{Recall} \\
& \textbf{Prevalence} & \textbf{(Pretrained)} & \textbf{(Pretrained)} \\
\hline
Distal Radius & 53.5\% & 87.7\% & 80.5\% \\
Distal Ulna & 17.9\% & 83.6\% & 79.6\% \\
Ulnar Styloid & 12.9\% & 83.2\% & 59.6\% \\
Scaphoid & 3.8\% & 33.3\% & 15.1\% \\
\hline
\end{tabular}
\end{table}

Figure \ref{fig:class_imbalance} visualizes the inverse relationship between class prevalence and detection accuracy. The performance gap between common and rare classes underscores the clinical risk: relying on overall metrics obscures catastrophic failures on minority classes that may carry disproportionate medical importance.

\begin{figure}[htbp]
\floatconts
  {fig:class_imbalance}
  {\caption{\textbf{Class prevalence inversely correlates with detection performance.} Scatterplot of training set prevalence vs. pretrained model mAP@0.5 for four bone types shows clear negative trend. Common fractures (distal radius, 53.5\%) achieve 81\% mAP while rare scaphoid (3.8\%) approaches zero. This performance gap motivates targeted intervention for rare, clinically critical classes.}}
  {\includegraphics[width=0.6\linewidth]{ims/fig_class_imbalance_NEW.png}}
\end{figure}

\subsection{Pipeline Component Performance}

\textbf{AP View Classification:} The ResNet-18 view classifier achieved 96.2\% test accuracy, with 95.8\% precision and 97.1\% recall for identifying AP views. High precision ensures that downstream bone detection operates exclusively on anatomically informative projections, while high recall maximizes data utilization.

\textbf{Bone Detection:} As shown in Table \ref{tab:bone_detection}, the bone YOLO achieved consistently high performance across all four anatomical classes (mean mAP@0.5 = 0.91). Notably, even scaphoid bones---the smallest and most challenging structure---were detected with 88\% mAP, confirming our hypothesis that bone localization is substantially easier than fracture detection due to consistent presence and structural invariance.

\textbf{LLM Report Extraction:} The LLM-based structured information extraction system demonstrated strong performance and practical scalability. Analysis across the full dataset showed 93\% schema compliance, with successfully structured responses containing all required fields (bone type, fracture location, pattern, healing status). Average processing time was 2.3 seconds per report, enabling high-throughput processing of tens of thousands of clinical documents without manual intervention. Manual verification of 250 randomly sampled cases yielded a hallucination rate of 0.98\%, indicating reliable generation of pseudo-ground-truth labels for downstream validation. Error analysis revealed that extraction challenges concentrated in reports with complex fracture patterns (15\% error rate) and ambiguous clinical language such as "questionable fracture" or "possible involvement" (12\% error rate). However, the combination of chain-of-thought reasoning, few-shot examples, and Pydantic schema validation successfully constrained the LLM to produce structured outputs suitable for automated pseudo-labeling. This performance establishes LLM-based report parsing as a viable alternative to manual annotation for creating training labels at scale.

\subsection{Pseudo-Label Curation Statistics}

Table \ref{tab:pseudolabel_stats} summarizes the automated label generation across bone classes. The co-localization pipeline processed 18,200 AP-filtered images, generating 14,500 true positive labels, 1,350 corrected false positives, and identifying 2,200 false negatives. Critically, manual annotation effort concentrated on rare classes: 75\% of scaphoid FNs received manual labels compared to only 15\% of radius FNs, efficiently allocating expert time to high-value cases. The curated dataset totals 15,850 training labels with only 150 manual annotations required.



\subsection{Finetuned Model Performance: Dramatic Improvement on Rare Classes}

Table \ref{tab:main_results} presents the core finding of this work: our multi-modal pseudo-labeling with focal loss training achieves substantial improvements on rare fracture classes while maintaining or improving performance on common classes. Scaphoid recall increases from 15.1\% (pretrained) to 67.4\% (ours), representing a 4.5× improvement and +52.3 percentage point absolute gain. Scaphoid precision improves from 33.3\% to 77.3\% (+44 points). Ulnar styloid shows +10.7 point recall gain (59.6\% → 66.9\%) and critically improves precision by +4.9 points (83.2\% → 88.1\%). Importantly, these gains do not sacrifice common class performance---distal radius recall improves from 80.5\% to 84.9\% (+4.4 points) with precision increasing from 87.7\% to 92.5\% (+4.8 points), while distal ulna achieves the strongest overall improvement (+8.6 point recall gain to 88.2\%). Figure \ref{fig:results} visualizes the performance improvements across different model configurations and bone types.

\begin{table}[htbp]
\centering
\caption{\textbf{Main Results: Multi-modal pseudo-labeling with focal loss dramatically improves detection across all fracture types.} Performance comparison across three training strategies on stratified test set (100 images per bone, 400 total). Our approach (rightmost column) achieves 4.5× improvement on scaphoid (3.8\% training prevalence) with +52.3 point recall gain, while simultaneously improving all other classes. The intermediate model without focal loss shows moderate gains, but focal loss training on pseudo-labeled data achieves superior results across all bone types.}
\label{tab:main_results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Bone Type} & \textbf{Training} & \textbf{Graz} & \textbf{Pseudo-label} & \textbf{Pseudo-label} \\
& \textbf{Prevalence} & \textbf{Pretrained} & \textbf{w/o Focal Loss} & \textbf{+ Focal Loss (Ours)} \\
\hline
\multicolumn{5}{|c|}{\textit{Recall (\%)}} \\
\hline
Distal Radius & 53.5\% & 80.5 & 71.7 & \textbf{84.9} \\
Distal Ulna & 17.9\% & 79.6 & 78.0 & \textbf{88.2} \\
Ulnar Styloid & 12.9\% & 59.6 & 63.9 & \textbf{66.9} \\
Scaphoid & 3.8\% & 15.1 & 44.2 & \textbf{67.4} \\
\hline
\textbf{Macro Avg} & --- & 58.7 & 64.5 & \textbf{76.9} \\
\hline
\hline
\multicolumn{5}{|c|}{\textit{Precision (\%)}} \\
\hline
Distal Radius & 53.5\% & 87.7 & 77.0 & \textbf{92.5} \\
Distal Ulna & 17.9\% & 83.6 & 82.9 & \textbf{86.8} \\
Ulnar Styloid & 12.9\% & 83.2 & 77.4 & \textbf{88.1} \\
Scaphoid & 3.8\% & 33.3 & 64.4 & \textbf{77.3} \\
\hline
\textbf{Macro Avg} & --- & 72.0 & 75.4 & \textbf{86.2} \\
\hline
\end{tabular}
\end{table}

\begin{figure}[htbp]
\floatconts
  {fig:results}
  {\caption{\textbf{Performance comparison across training approaches.} }}
  {\includegraphics[width=1.0\linewidth]{ims/fig33.png}}
\end{figure}



\begin{figure}[htbp]
\floatconts
  {fig:results}
  {\caption{\textbf{Data curation pipeline for training set generation}. Raw dataset undergoes filtering (no cast/hardware, AP views only), followed by parallel processing with bone detection, pretrained fracture detection, and LLM report parsing. Co-localization validates predictions against ground truth, splitting into three branches. As an example of scaphoid bone fractures -  True Positives (kept as-is, 14,500), False Positives (boxes removed, 1,350), and False Negatives (150 manually annotated, prioritizing rare bones). Final curated dataset: 15,850 labels with 400-image stratified test set held out.}}
  {\includegraphics[width=1.0\linewidth]{ims/mysvg.png}}
\end{figure}


\subsection{Ablation Study: Contribution of Pipeline Components}

Table \ref{tab:ablation} quantifies the individual contribution of each pipeline component. Finetuning on pseudo-labels alone (without focal loss) provides moderate gains on rare classes. Adding focal loss yields substantial additional improvement, particularly for scaphoid ([+XX\%] mAP). The AP view filter and bone co-localization together enable accurate pseudo-labeling, as evidenced by the large gap between pretrained baseline and finetuned models.

% \begin{table}[htbp]
% \centering
% \caption{\textbf{Ablation study quantifies contribution of pipeline components.} Progressive addition of components shows cumulative improvements. Pseudo-labeling via co-localization provides substantial gains; focal loss adds further improvement by emphasizing rare, hard examples.}
% \label{tab:ablation}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Configuration} & \textbf{Radius} & \textbf{Ulna} & \textbf{Styloid} & \textbf{Scaphoid} \\
% & \textbf{mAP@0.5} & \textbf{mAP@0.5} & \textbf{mAP@0.5} & \textbf{mAP@0.5} \\
% \hline
% Pretrained (Graz) & [0.XX] & [0.XX] & [0.XX] & $\sim$0.00 \\
% \hline
% + Pseudo-labels (no focal) & [0.XX] & [0.XX] & [0.XX] & [0.XX] \\
% + Focal loss (ours) & \textbf{[0.XX]} & \textbf{[0.XX]} & \textbf{[0.XX]} & \textbf{[0.XX]} \\
% \hline
% \textit{Absolute gain (ours - pretrained)} & [+0.XX] & [+0.XX] & [+0.XX] & [+0.XX] \\
% \hline
% \end{tabular}
% \end{table}

\begin{comment}
\begin{table}[]
\begin{tabular}{|c|c|c|}
\hline
\textbf{LLM Evaluation Metric} & \textbf{Hallucination Rate} & \textbf{Cases Analyzed} \\ \hline
Text-based Heuristics      & 0.98\%                      & 12,437                  \\ \hline
Confidence Scoring         & 0.31\%                      & 9,515                   \\ \hline
\end{tabular}
\centering
\caption{Hallucination rate analysis of the LLM-based structured information extraction system using two complementary validation methods. Results are based on 12,437 total findings across 8,040 unique cases, with 11,241 fracture detections and 1,196 no-fracture findings. Confidence scoring refers to LLM detections with uncertain language in the report based on 4 level  criteria. Text based heuristics are quantified contradictions between LLM detected fractures and report language. }
\label{tab:hallucination_rates} 
\end{table}
\end{comment}

\begin{comment}
\begin{table}[]
\begin{tabular}{|ll|}
\hline
\multicolumn{2}{|l|}{\textbf{Confidence Distribution}}              \\ \hline
\multicolumn{1}{|l|}{High Confidence Detections}   & 9,481 (84.3\%) \\ \hline
\multicolumn{1}{|l|}{Medium Confidence Detections} & 1,400 (12.4\%) \\ \hline
\multicolumn{1}{|l|}{Low Confidence Detections}    & 34 (0.3\%)     \\ \hline
\multicolumn{2}{|l|}{\textbf{Error Analysis}}                       \\ \hline
\multicolumn{1}{|l|}{Potential Hallucinations}     & 120 (0.96\%)   \\ \hline
\multicolumn{1}{|l|}{Potential Misses}             & 101 (0.81\%)   \\ \hline
\multicolumn{1}{|l|}{Rare Combinations}            & 45 (0.40\%)    \\ \hline
\end{tabular}
\centering
\caption{LLM-based structured extraction system performance, showing distribution of confidence levels and errors across the test set.}
\label{tab:llm_performance_stats}
\end{table}
\end{comment}



\subsection{Qualitative Results: Enhanced Detection Across All Fracture Types}

Figure \ref{fig:qualitative} presents representative examples demonstrating the comprehensive improvements achieved by our multi-modal pseudo-labeling approach. Row 1 shows a scaphoid fracture that was completely missed by the pretrained model but successfully localized after training with our automated pseudo-labels, demonstrating the system's capability on rare fractures. Row 2 illustrates an ulnar styloid fracture with improved detection accuracy. Row 3 demonstrates strong performance on common distal radius fractures, showing that our approach maintains high accuracy on frequent presentations while also improving rare case detection. Row 4 shows a challenging multi-fracture case with overlapping bones and cast artifacts; our model correctly identifies both distal radius and ulna fractures while avoiding false positives that affected the pretrained model in the metacarpal region. These examples illustrate the robustness of integrating LLM-extracted report information, anatomical bone localization, and VLM validation for comprehensive fracture detection.

% \begin{figure}[!ht]
% \floatconts
%   {fig:qualitative}
%   {\caption{\textbf{Qualitative comparison shows successful rare fracture detection.} Representative examples from test set comparing ground truth (green), pretrained Graz model (red), and our finetuned model (blue). Row 1: Scaphoid fracture missed entirely by pretrained model is correctly detected after finetuning (critical clinical improvement). Row 2: Ulnar styloid fracture similarly recovered. Row 3: Common distal radius fracture maintains high accuracy. Row 4: Complex multi-fracture case with cast artifacts---our model avoids false positive in metacarpals while correctly detecting both radius and ulna fractures.}}
%   {\includegraphics[width=1.0\linewidth]{ims/fig_qualitative_NEW.png}}
% \end{figure}

% Figure \ref{fig:bone_detection_examples} visualizes the bone detection step of our pipeline, showing bounding boxes for all four anatomical structures across diverse patient anatomies. Consistent, high-quality bone localization enables reliable downstream co-localization and pseudo-label validation.

% \begin{figure}[htbp]
% \floatconts
%   {fig:bone_detection_examples}
%   {\caption{\textbf{Bone detection examples demonstrate reliable anatomical localization.} Bone YOLO detections (color-coded by type: radius=red, ulna=blue, ulnar styloid=green, scaphoid=yellow) across diverse patient presentations. High accuracy across all four bone types, including small scaphoid, enables reliable fracture-bone co-localization for pseudo-label generation.}}
%   {\includegraphics[width=0.8\linewidth]{ims/fig_bone_examples_NEW.png}}
% \end{figure}



\section{Discussion}

\subsection{Principal Findings}

This work demonstrates that multi-modal pseudo-labeling combining LLMs, VLMs, and anatomical knowledge can significantly enhance fracture detection across all fracture types without requiring extensive manual annotation. Our automated pipeline achieves substantial performance improvements: macro-averaged recall increases from 58.7\% to 76.9\% and precision from 72.0\% to 86.2\% on a stratified test set of 400 images. These improvements span both common fractures (distal radius) and rare but clinically critical presentations (scaphoid fracture recall: 15.1\% → 67.4\%, a 4.5× improvement; precision: 33.3\% → 77.3\%, a 2.3× improvement).

The success of our approach rests on integrating multiple complementary information sources: (1) LLM-based extraction of structured fracture information from clinical reports using chain-of-thought reasoning provides ground-truth labels without manual annotation; (2) anatomical bone detection provides spatial context and enables validation through co-localization; (3) VLM verification adds an additional layer of confirmation for anatomical correctness; and (4) focal loss training effectively emphasizes challenging cases during model optimization. By leveraging the rich information naturally present in clinical workflows, we overcome the prohibitive cost of exhaustive manual labeling while achieving strong performance across the full spectrum of fracture presentations.

\subsection{Comparison to Prior Work}

Prior work in medical fracture detection has primarily focused on developing deep learning architectures or transfer learning strategies, but often suffers from limited training data and class imbalances. While public datasets like GRAZPEDWRI-DX \cite{nagy2022pediatric} provide valuable resources, models trained on these datasets may not generalize well to new clinical populations and struggle with underrepresented fracture types.

Recent pseudo-labeling approaches \cite{keuth2024sam, mazurowski2023segment} have explored weakly supervised methods to reduce annotation burden, but typically do not integrate clinical documentation or validate predictions against radiology reports. The emergence of LLMs and VLMs presents new opportunities for medical image analysis. Recent work has shown that LLMs can extract structured information from clinical text \cite{nori2023medprompt} and VLMs can provide anatomical understanding \cite{almohamad2025vlm}. However, these modalities have not been systematically integrated for object detection training.

Our contribution builds on these advances by developing an end-to-end pipeline that combines: (1) LLM-based report parsing with prompt engineering and validation safeguards; (2) anatomical knowledge through bone detection for spatial validation; (3) VLM verification for visual confirmation; and (4) targeted training with focal loss. This multi-modal integration enables automated generation of high-quality pseudo-labels that improve detection performance across the full spectrum of fracture presentations, from common to rare cases.

\subsection{Clinical Implications}

Enhanced fracture detection across all fracture types has direct implications for patient care. An AI system that achieves 76.9\% macro-averaged recall and 86.2\% precision could serve as a valuable decision support tool in busy emergency departments, providing consistent second-reader functionality and reducing diagnostic errors.

The improvements in rare fracture detection are particularly clinically meaningful. Scaphoid fractures, though less frequent, can progress to avascular necrosis if missed, leading to permanent disability. An AI system achieving 67.4\% scaphoid recall (compared to 15.1\% baseline) could substantially reduce missed diagnoses. Similarly, improved detection of ulnar styloid fractures, which can affect pediatric growth plates, helps prevent potential growth disturbances.

More broadly, our approach demonstrates that automated pseudo-labeling can leverage the structured information already present in clinical workflows—radiology reports, anatomical relationships, and visual confirmation—to create robust detection systems without prohibitive manual annotation costs. This has important implications for scaling AI deployment across medical imaging applications, particularly in scenarios where expert annotation is scarce or expensive. The multi-modal validation strategy (LLM + anatomical context + VLM) provides a template for developing reliable pseudo-labels that can improve model performance across diverse clinical presentations.

\subsection{Limitations}

Several limitations warrant discussion. First, manual annotation remains necessary for a subset of false negatives (150 cases), particularly for rare scaphoid fractures. While far less than exhaustive annotation (15,850 total labels generated), this represents a residual labeling cost. Future work could explore active learning to minimize manual FN labeling by prioritizing maximally informative cases.

Second, our pipeline assumes availability of radiology reports with accurate fracture descriptions. Report quality varies across institutions, and ambiguous language ("questionable fracture," "possible scaphoid involvement") reduces LLM extraction reliability. We observed 0.98\% hallucination rate and 2.8\% extraction errors for complex cases, introducing noise into pseudo-labels. More sophisticated report parsing or multi-LLM ensembling may improve robustness.

Third, bone detection errors propagate downstream. While our bone YOLO achieves strong performance across all anatomical structures, occasional mislocalization (especially for small scaphoid in low-quality images) causes incorrect fracture-bone assignments. A joint bone-fracture detection model trained end-to-end may mitigate this issue.

Fourth, our approach is demonstrated only for pediatric wrist fractures. Generalization to other anatomical sites (e.g., ankle, elbow, hip) requires retraining the bone detector and adapting anatomical knowledge for co-localization. However, the overall pipeline design---AP filtering, anatomical localization, co-localization validation, focal loss training---should transfer broadly.

Finally, while our stratified test set of 400 images ensures balanced representation with 100 cases per bone type (100 scaphoid, 100 distal radius, 100 distal ulna, 100 ulnar styloid), larger prospective validation studies are needed before clinical deployment to fully assess performance across diverse patient populations and imaging conditions.

\subsection{Future Directions}

Several extensions could strengthen this work. First, incorporating uncertainty estimation (e.g., Monte Carlo dropout, ensembling) would allow the model to flag low-confidence predictions for human review, particularly for rare classes where errors carry high clinical cost. Second, extending to additional fracture types (metacarpals, phalanges, carpal bones beyond scaphoid) would provide comprehensive wrist fracture detection. Third, joint training of bone detection and fracture detection networks in a multi-task framework could improve both tasks through shared representations and reduce error propagation.

Fourth, applying this approach to other imbalanced medical detection problems (lung nodule subtypes, rare retinal pathologies, uncommon brain lesions) would test generalizability and establish best practices for bone-aware pseudo-labeling in diverse anatomical contexts. Fifth, prospective clinical trials measuring diagnostic accuracy, radiologist workflow efficiency, and patient outcomes (missed fracture rates, time to diagnosis) are essential for translation to clinical practice.

\subsection{Conclusions}

We demonstrate that multi-modal pseudo-labeling combining LLMs, VLMs, and anatomical knowledge significantly enhances pediatric wrist fracture detection without requiring extensive manual annotation. Our automated pipeline achieves macro-averaged recall of 76.9\% and precision of 86.2\%, compared to 58.7\% and 72.0\% for pretrained baselines. Performance improvements are consistent across all fracture types, including both common presentations and rare but clinically critical cases (scaphoid: 15.1\% → 67.4\% recall; ulnar styloid: improved metrics).

By integrating complementary information sources—LLM-extracted report data, anatomical bone detection, and VLM visual verification—we overcome the annotation bottleneck that limits medical object detection development. The approach leverages information naturally present in clinical workflows, making it scalable and cost-effective. This work establishes the feasibility of automated pseudo-labeling for medical imaging and provides a framework that can be extended to other anatomical regions and detection tasks. Future work should focus on prospective clinical validation, extension to additional fracture types and anatomical sites, and development of uncertainty estimation mechanisms for clinical deployment.

% We demonstrated that LLM-powered information extraction from radiology reports can effectively enhance fracture detection models, achieving superior performance (mAP 0.822) compared to manual annotation approaches without requiring additional expert labeling. Our pseudo-blind finetuning method particularly excelled with pediatric cases where growth plates typically challenge traditional detection systems. While the approach showed promise, limitations emerged with complex fracture patterns (15\% error rate) and report language ambiguity (12\% error rate). Future work should focus on improving robustness to report variability and extending the method to other anatomical regions. This study establishes the feasibility of using LLM-extracted knowledge to reduce annotation burden while maintaining high diagnostic accuracy in clinical settings.
\clearpage 

\bibliography{midl-samplebibliography}

\end{document}
